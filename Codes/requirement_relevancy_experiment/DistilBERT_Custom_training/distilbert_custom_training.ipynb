{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training of DistilBERT\n",
    "\n",
    "In this notebook, we will custom train the DistilBERT model for squence classsification task and then use the trained model to make predictions on the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../../../Datasets/pure/PURE_train.csv\", engine=\"pyarrow\", usecols=[\"Requirement\", \"label\"])\n",
    "test_df = pd.read_csv(\"../../../Datasets/pure/PURE_test.csv\", engine=\"pyarrow\", usecols=[\"Requirement\", \"label\"])\n",
    "valid_df = pd.read_csv(\"../../../Datasets/pure/PURE_valid.csv\", engine=\"pyarrow\", usecols=[\"Requirement\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_df[\"Requirement\"].to_numpy()\n",
    "train_y = train_df[\"label\"].to_numpy()\n",
    "\n",
    "test_X = test_df[\"Requirement\"].to_numpy()\n",
    "test_y = test_df[\"label\"].to_numpy()\n",
    "\n",
    "valid_X = valid_df[\"Requirement\"].to_numpy()\n",
    "valid_y = valid_df[\"label\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting To Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequirementsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\SWE Class\\Github Desktop\\Classification of Software Requirements\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_X.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_X.tolist(), truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_X.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_req_dataset = RequirementsDataset(train_encodings, train_y)\n",
    "test_req_dataset = RequirementsDataset(test_encodings, test_y)\n",
    "valid_req_dataset = RequirementsDataset(valid_encodings, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1996,  5576,  2323,  3073,  6851,  6123,  1011,  7591,  2393,\n",
       "          3430,  2005,  2035,  1996,  2825,  4506,  1998, 16820,  2006,  2035,\n",
       "          5310, 19706,  1999,  1996,  4646,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_req_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model (With Pytorch and CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MY_DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\SWE Class\\Github Desktop\\Classification of Software Requirements\\venv\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.7053360939025879\n",
      "Epoch 0 Loss: 0.6753596663475037\n",
      "Epoch 0 Loss: 0.6532688140869141\n",
      "Epoch 0 Loss: 0.6961690187454224\n",
      "Epoch 0 Loss: 0.6172416806221008\n",
      "Epoch 0 Loss: 0.5248299837112427\n",
      "Epoch 0 Loss: 0.5229129195213318\n",
      "Epoch 0 Loss: 0.6888538599014282\n",
      "Epoch 0 Loss: 0.6065130829811096\n",
      "Epoch 0 Loss: 0.649664044380188\n",
      "Epoch 0 Loss: 0.5263355374336243\n",
      "Epoch 0 Loss: 0.4570524990558624\n",
      "Epoch 0 Loss: 0.6019513010978699\n",
      "Epoch 0 Loss: 0.4622551202774048\n",
      "Epoch 0 Loss: 0.6873756051063538\n",
      "Epoch 0 Loss: 0.4956197142601013\n",
      "Epoch 0 Loss: 0.4668370187282562\n",
      "Epoch 0 Loss: 0.3469903767108917\n",
      "Epoch 0 Loss: 0.35774239897727966\n",
      "Epoch 0 Loss: 0.5159806609153748\n",
      "Epoch 0 Loss: 0.4887233078479767\n",
      "Epoch 0 Loss: 0.331891268491745\n",
      "Epoch 0 Loss: 0.5228985548019409\n",
      "Epoch 0 Loss: 0.5415565371513367\n",
      "Epoch 0 Loss: 0.3013581931591034\n",
      "Epoch 0 Loss: 0.5354827046394348\n",
      "Epoch 0 Loss: 0.4932573735713959\n",
      "Epoch 0 Loss: 0.23166301846504211\n",
      "Epoch 0 Loss: 0.31125345826148987\n",
      "Epoch 0 Loss: 0.4800143837928772\n",
      "Epoch 0 Loss: 0.23434975743293762\n",
      "Epoch 0 Loss: 0.6767332553863525\n",
      "Epoch 0 Loss: 0.6986562013626099\n",
      "Epoch 0 Loss: 0.270342618227005\n",
      "Epoch 0 Loss: 0.28005650639533997\n",
      "Epoch 0 Loss: 0.29957297444343567\n",
      "Epoch 0 Loss: 0.23873937129974365\n",
      "Epoch 0 Loss: 0.4653693437576294\n",
      "Epoch 0 Loss: 0.38006433844566345\n",
      "Epoch 0 Loss: 0.3242231011390686\n",
      "Epoch 0 Loss: 0.3364027440547943\n",
      "Epoch 0 Loss: 0.3622778356075287\n",
      "Epoch 0 Loss: 0.5055057406425476\n",
      "Epoch 0 Loss: 0.5130198001861572\n",
      "Epoch 0 Loss: 0.7299600839614868\n",
      "Epoch 0 Loss: 0.22649268805980682\n",
      "Epoch 0 Loss: 0.27544525265693665\n",
      "Epoch 0 Loss: 0.2256559580564499\n",
      "Epoch 0 Loss: 0.39557257294654846\n",
      "Epoch 0 Loss: 0.4986792504787445\n",
      "Epoch 0 Loss: 0.18557122349739075\n",
      "Epoch 0 Loss: 0.23898088932037354\n",
      "Epoch 0 Loss: 0.2580222189426422\n",
      "Epoch 0 Loss: 0.2795729637145996\n",
      "Epoch 0 Loss: 0.3191413879394531\n",
      "Epoch 0 Loss: 0.2948929965496063\n",
      "Epoch 0 Loss: 0.2389093041419983\n",
      "Epoch 0 Loss: 0.6716707944869995\n",
      "Epoch 0 Loss: 0.12038330733776093\n",
      "Epoch 0 Loss: 0.5091865062713623\n",
      "Epoch 0 Loss: 0.20987588167190552\n",
      "Epoch 0 Loss: 0.43863317370414734\n",
      "Epoch 0 Loss: 0.2506013810634613\n",
      "Epoch 0 Loss: 0.39690065383911133\n",
      "Epoch 0 Loss: 0.17783692479133606\n",
      "Epoch 0 Loss: 0.272417277097702\n",
      "Epoch 0 Loss: 0.2381972074508667\n",
      "Epoch 0 Loss: 0.45511701703071594\n",
      "Epoch 0 Loss: 0.4481271207332611\n",
      "Epoch 0 Loss: 0.26252949237823486\n",
      "Epoch 0 Loss: 0.1512838751077652\n",
      "Epoch 0 Loss: 0.41915348172187805\n",
      "Epoch 0 Loss: 0.32031726837158203\n",
      "Epoch 0 Loss: 0.1068166196346283\n",
      "Epoch 0 Loss: 0.4439477324485779\n",
      "Epoch 0 Loss: 0.40124043822288513\n",
      "Epoch 0 Loss: 0.2972921133041382\n",
      "Epoch 0 Loss: 0.29938066005706787\n",
      "Epoch 0 Loss: 0.49698972702026367\n",
      "Epoch 0 Loss: 0.21877598762512207\n",
      "Epoch 0 Loss: 0.24293585121631622\n",
      "Epoch 0 Loss: 0.44642868638038635\n",
      "Epoch 0 Loss: 0.19752512872219086\n",
      "Epoch 0 Loss: 0.2576047480106354\n",
      "Epoch 0 Loss: 0.23750437796115875\n",
      "Epoch 0 Loss: 0.18409210443496704\n",
      "Epoch 0 Loss: 0.544273316860199\n",
      "Epoch 0 Loss: 0.307489275932312\n",
      "Epoch 0 Loss: 0.19133135676383972\n",
      "Epoch 0 Loss: 0.09508116543292999\n",
      "Epoch 0 Loss: 0.4375496804714203\n",
      "Epoch 0 Loss: 0.2643873691558838\n",
      "Epoch 0 Loss: 0.29468491673469543\n",
      "Epoch 0 Loss: 0.15220943093299866\n",
      "Epoch 0 Loss: 0.16181553900241852\n",
      "Epoch 0 Loss: 0.2773746848106384\n",
      "Epoch 0 Loss: 0.18294093012809753\n",
      "Epoch 0 Loss: 0.32127219438552856\n",
      "Epoch 0 Loss: 0.5718586444854736\n",
      "Epoch 0 Loss: 0.19979864358901978\n",
      "Epoch 0 Loss: 0.3542678952217102\n",
      "Epoch 0 Loss: 0.08228253573179245\n",
      "Epoch 0 Loss: 0.14988180994987488\n",
      "Epoch 0 Loss: 0.3136819005012512\n",
      "Epoch 0 Loss: 0.1640620231628418\n",
      "Epoch 0 Loss: 0.237115278840065\n",
      "Epoch 0 Loss: 0.312684565782547\n",
      "Epoch 0 Loss: 0.4467197060585022\n",
      "Epoch 0 Loss: 0.2107933908700943\n",
      "Epoch 0 Loss: 0.6904471516609192\n",
      "Epoch 0 Loss: 0.27750319242477417\n",
      "Epoch 0 Loss: 0.2196771800518036\n",
      "Epoch 0 Loss: 0.11708652973175049\n",
      "Epoch 0 Loss: 0.30094805359840393\n",
      "Epoch 0 Loss: 0.23358114063739777\n",
      "Epoch 0 Loss: 0.16976049542427063\n",
      "Epoch 0 Loss: 0.2941210865974426\n",
      "Epoch 0 Loss: 0.12185151129961014\n",
      "Epoch 0 Loss: 0.16308385133743286\n",
      "Epoch 0 Loss: 0.1466427445411682\n",
      "Epoch 0 Loss: 0.15093950927257538\n",
      "Epoch 0 Loss: 0.4017191529273987\n",
      "Epoch 0 Loss: 0.22783300280570984\n",
      "Epoch 0 Loss: 0.7302539348602295\n",
      "Epoch 0 Loss: 0.4564652442932129\n",
      "Epoch 0 Loss: 0.38307318091392517\n",
      "Epoch 0 Loss: 0.49548014998435974\n",
      "Epoch 0 Loss: 0.11602552235126495\n",
      "Epoch 0 Loss: 0.23594266176223755\n",
      "Epoch 0 Loss: 0.11155243963003159\n",
      "Epoch 0 Loss: 0.22310228645801544\n",
      "Epoch 0 Loss: 0.25778263807296753\n",
      "Epoch 0 Loss: 0.2501585781574249\n",
      "Epoch 0 Loss: 0.5016618371009827\n",
      "Epoch 0 Loss: 0.12225431948900223\n",
      "Epoch 0 Loss: 0.5327563881874084\n",
      "Epoch 0 Loss: 0.2772536277770996\n",
      "Epoch 0 Loss: 0.22060661017894745\n",
      "Epoch 0 Loss: 0.3791542053222656\n",
      "Epoch 0 Loss: 0.3117993474006653\n",
      "Epoch 0 Loss: 0.18091997504234314\n",
      "Epoch 0 Loss: 0.14113952219486237\n",
      "Epoch 0 Loss: 0.17860659956932068\n",
      "Epoch 0 Loss: 0.19168947637081146\n",
      "Epoch 0 Loss: 0.14431455731391907\n",
      "Epoch 0 Loss: 0.18886958062648773\n",
      "Epoch 0 Loss: 0.18629449605941772\n",
      "Epoch 0 Loss: 0.44598647952079773\n",
      "Epoch 0 Loss: 0.42567479610443115\n",
      "Epoch 0 Loss: 0.1300608217716217\n",
      "Epoch 0 Loss: 0.2873770296573639\n",
      "Epoch 0 Loss: 0.15583641827106476\n",
      "Epoch 0 Loss: 0.46321672201156616\n",
      "Epoch 0 Loss: 0.15428416430950165\n",
      "Epoch 0 Loss: 0.2784711718559265\n",
      "Epoch 0 Loss: 0.16310839354991913\n",
      "Epoch 0 Loss: 0.11820969730615616\n",
      "Epoch 0 Loss: 0.2811683416366577\n",
      "Epoch 0 Loss: 0.32439348101615906\n",
      "Epoch 0 Loss: 0.17761783301830292\n",
      "Epoch 0 Loss: 0.3084890842437744\n",
      "Epoch 0 Loss: 0.17349955439567566\n",
      "Epoch 0 Loss: 0.29564428329467773\n",
      "Epoch 0 Loss: 0.38381412625312805\n",
      "Epoch 0 Loss: 0.14026649296283722\n",
      "Epoch 0 Loss: 0.06285851448774338\n",
      "Epoch 0 Loss: 0.14134769141674042\n",
      "Epoch 0 Loss: 0.1616806536912918\n",
      "Epoch 0 Loss: 0.2654598653316498\n",
      "Epoch 0 Loss: 0.21648138761520386\n",
      "Epoch 0 Loss: 0.2611435055732727\n",
      "Epoch 0 Loss: 0.2537262439727783\n",
      "Epoch 0 Loss: 0.4020870327949524\n",
      "Epoch 0 Loss: 0.30531516671180725\n",
      "Epoch 0 Loss: 0.1649550497531891\n",
      "Epoch 0 Loss: 0.3045163154602051\n",
      "Epoch 0 Loss: 0.2659551501274109\n",
      "Epoch 0 Loss: 0.3076062798500061\n",
      "Epoch 0 Loss: 0.4762643277645111\n",
      "Epoch 0 Loss: 0.22567465901374817\n",
      "Epoch 0 Loss: 0.1951836794614792\n",
      "Epoch 0 Loss: 0.15004143118858337\n",
      "Epoch 0 Loss: 0.18488824367523193\n",
      "Epoch 0 Loss: 0.43716001510620117\n",
      "Epoch 0 Loss: 0.1434265524148941\n",
      "Epoch 0 Loss: 0.4807574450969696\n",
      "Epoch 0 Loss: 0.3060168921947479\n",
      "Epoch 0 Loss: 0.3157493472099304\n",
      "Epoch 0 Loss: 0.08621160686016083\n",
      "Epoch 0 Loss: 0.2673110067844391\n",
      "Epoch 0 Loss: 0.21933704614639282\n",
      "Epoch 0 Loss: 0.3604141175746918\n",
      "Epoch 0 Loss: 0.18679043650627136\n",
      "Epoch 0 Loss: 0.3108275532722473\n",
      "Epoch 0 Loss: 0.19748391211032867\n",
      "Epoch 0 Loss: 0.574919581413269\n",
      "Epoch 0 Loss: 0.3016161620616913\n",
      "Epoch 0 Loss: 0.36649662256240845\n",
      "Epoch 0 Loss: 0.10348864644765854\n",
      "Epoch 0 Loss: 0.12856192886829376\n",
      "Epoch 0 Loss: 0.058031823486089706\n",
      "Epoch 0 Loss: 0.5549975633621216\n",
      "Epoch 0 Loss: 0.3663358986377716\n",
      "Epoch 0 Loss: 0.1389794647693634\n",
      "Epoch 0 Loss: 0.08285179734230042\n",
      "Epoch 0 Loss: 0.22546625137329102\n",
      "Epoch 0 Loss: 0.30362623929977417\n",
      "Epoch 0 Loss: 0.3782344162464142\n",
      "Epoch 0 Loss: 0.0982988178730011\n",
      "Epoch 0 Loss: 0.22042053937911987\n",
      "Epoch 0 Loss: 0.1440671682357788\n",
      "Epoch 0 Loss: 0.44798269867897034\n",
      "Epoch 0 Loss: 0.2934234142303467\n",
      "Epoch 0 Loss: 0.09146188199520111\n",
      "Epoch 0 Loss: 0.43270301818847656\n",
      "Epoch 0 Loss: 0.036782898008823395\n",
      "Epoch 0 Loss: 0.032262127846479416\n",
      "Epoch 0 Loss: 0.10880416631698608\n",
      "Epoch 0 Loss: 0.22228968143463135\n",
      "Epoch 0 Loss: 0.14167018234729767\n",
      "Epoch 0 Loss: 0.1106390655040741\n",
      "Epoch 0 Loss: 0.060352224856615067\n",
      "Epoch 0 Loss: 0.44778093695640564\n",
      "Epoch 0 Loss: 0.11317233741283417\n",
      "Epoch 0 Loss: 0.4433938264846802\n",
      "Epoch 0 Loss: 0.44427230954170227\n",
      "Epoch 0 Loss: 0.31610679626464844\n",
      "Epoch 0 Loss: 0.3245171308517456\n",
      "Epoch 0 Loss: 0.061850421130657196\n",
      "Epoch 0 Loss: 0.2399188131093979\n",
      "Epoch 0 Loss: 0.260940819978714\n",
      "Epoch 0 Loss: 0.17092086374759674\n",
      "Epoch 0 Loss: 0.42892831563949585\n",
      "Epoch 0 Loss: 0.1896039992570877\n",
      "Epoch 0 Loss: 0.10503464937210083\n",
      "Epoch 0 Loss: 0.2531038224697113\n",
      "Epoch 0 Loss: 0.5337070226669312\n",
      "Epoch 0 Loss: 0.19824343919754028\n",
      "Epoch 0 Loss: 0.12918587028980255\n",
      "Epoch 0 Loss: 0.4709280729293823\n",
      "Epoch 0 Loss: 0.28430965542793274\n",
      "Epoch 0 Loss: 0.10378725826740265\n",
      "Epoch 0 Loss: 0.12630748748779297\n",
      "Epoch 0 Loss: 0.2415851205587387\n",
      "Epoch 0 Loss: 0.15253578126430511\n",
      "Epoch 0 Loss: 0.3448362350463867\n",
      "Epoch 0 Loss: 0.18898169696331024\n",
      "Epoch 0 Loss: 0.2793956398963928\n",
      "Epoch 0 Loss: 0.21306490898132324\n",
      "Epoch 0 Loss: 0.37500542402267456\n",
      "Epoch 0 Loss: 0.13396897912025452\n",
      "Epoch 0 Loss: 0.26498734951019287\n",
      "Epoch 0 Loss: 0.057913996279239655\n",
      "Epoch 0 Loss: 0.3153080344200134\n",
      "Epoch 0 Loss: 0.42522066831588745\n",
      "Epoch 0 Loss: 0.45518550276756287\n",
      "Epoch 0 Loss: 0.2607111632823944\n",
      "Epoch 0 Loss: 0.0972222238779068\n",
      "Epoch 0 Loss: 0.49415528774261475\n",
      "Epoch 0 Loss: 0.22965092957019806\n",
      "Epoch 0 Loss: 0.2636451721191406\n",
      "Epoch 0 Loss: 0.21381761133670807\n",
      "Epoch 0 Loss: 0.12279213964939117\n",
      "Epoch 0 Loss: 0.35966020822525024\n",
      "Epoch 0 Loss: 0.31844356656074524\n",
      "Epoch 0 Loss: 0.756131112575531\n",
      "Epoch 0 Loss: 0.160203218460083\n",
      "Epoch 0 Loss: 0.10531497746706009\n",
      "Epoch 0 Loss: 0.2588658630847931\n",
      "Epoch 0 Loss: 0.13362844288349152\n",
      "Epoch 0 Loss: 0.24253714084625244\n",
      "Epoch 0 Loss: 0.17992159724235535\n",
      "Epoch 0 Loss: 0.18144990503787994\n",
      "Epoch 0 Loss: 0.12726004421710968\n",
      "Epoch 0 Loss: 0.15488916635513306\n",
      "Epoch 0 Loss: 0.1993325650691986\n",
      "Epoch 0 Loss: 0.6341073513031006\n",
      "Epoch 0 Loss: 0.3594283163547516\n",
      "Epoch 0 Loss: 0.15437956154346466\n",
      "Epoch 0 Loss: 0.48898908495903015\n",
      "Epoch 0 Loss: 0.21040800213813782\n",
      "Epoch 0 Loss: 0.1980181336402893\n",
      "Epoch 0 Loss: 0.4405449330806732\n",
      "Epoch 0 Loss: 0.3449598550796509\n",
      "Epoch 0 Loss: 0.39443302154541016\n",
      "Epoch 0 Loss: 0.3635992705821991\n",
      "Epoch 0 Loss: 0.5352138876914978\n",
      "Epoch 0 Loss: 0.13153021037578583\n",
      "Epoch 0 Loss: 0.17606578767299652\n",
      "Epoch 0 Loss: 0.08238065242767334\n",
      "Epoch 0 Loss: 0.3995278775691986\n",
      "Epoch 0 Loss: 0.20562338829040527\n",
      "Epoch 0 Loss: 0.14250577986240387\n",
      "Epoch 0 Loss: 0.39694860577583313\n",
      "Epoch 0 Loss: 0.5856835246086121\n",
      "Epoch 0 Loss: 0.6020902991294861\n",
      "Epoch 0 Loss: 0.15609805285930634\n",
      "Epoch 0 Loss: 0.10044453293085098\n",
      "Epoch 0 Loss: 0.22627519071102142\n",
      "Epoch 0 Loss: 0.15110404789447784\n",
      "Epoch 0 Loss: 0.16582216322422028\n",
      "Epoch 0 Loss: 0.3656400442123413\n",
      "Epoch 0 Loss: 0.16269828379154205\n",
      "Epoch 0 Loss: 0.20709183812141418\n",
      "Epoch 0 Loss: 0.47321590781211853\n",
      "Epoch 0 Loss: 0.21663522720336914\n",
      "Epoch 0 Loss: 0.17650790512561798\n",
      "Epoch 0 Loss: 0.09928786754608154\n",
      "Epoch 0 Loss: 0.2908015251159668\n",
      "Epoch 0 Loss: 0.10417202115058899\n",
      "Epoch 0 Loss: 0.36386799812316895\n",
      "Epoch 0 Loss: 0.09630661457777023\n",
      "Epoch 0 Loss: 0.12408348172903061\n",
      "Epoch 0 Loss: 0.2762832045555115\n",
      "Epoch 0 Loss: 0.21178634464740753\n",
      "Epoch 0 Loss: 0.1836443692445755\n",
      "Epoch 0 Loss: 0.10446295142173767\n",
      "Epoch 0 Loss: 0.2730008661746979\n",
      "Epoch 0 Loss: 0.18235084414482117\n",
      "Epoch 0 Loss: 0.06731806695461273\n",
      "Epoch 0 Loss: 0.42813840508461\n",
      "Epoch 0 Loss: 0.692409873008728\n",
      "Epoch 0 Loss: 0.1825963258743286\n",
      "Epoch 0 Loss: 0.16583597660064697\n",
      "Epoch 0 Loss: 0.18194328248500824\n",
      "Epoch 0 Loss: 0.19658958911895752\n",
      "Epoch 0 Loss: 0.15089000761508942\n",
      "Epoch 0 Loss: 0.15676230192184448\n",
      "Epoch 0 Loss: 0.11317513883113861\n",
      "Epoch 0 Loss: 0.14050883054733276\n",
      "Epoch 0 Loss: 0.1898888349533081\n",
      "Epoch 0 Loss: 0.056533586233854294\n",
      "Epoch 1 Loss: 0.09880111366510391\n",
      "Epoch 1 Loss: 0.0923011526465416\n",
      "Epoch 1 Loss: 0.19969558715820312\n",
      "Epoch 1 Loss: 0.11870688945055008\n",
      "Epoch 1 Loss: 0.06200120598077774\n",
      "Epoch 1 Loss: 0.15669886767864227\n",
      "Epoch 1 Loss: 0.05037516728043556\n",
      "Epoch 1 Loss: 0.14764340221881866\n",
      "Epoch 1 Loss: 0.0715535432100296\n",
      "Epoch 1 Loss: 0.17031288146972656\n",
      "Epoch 1 Loss: 0.38598233461380005\n",
      "Epoch 1 Loss: 0.20658744871616364\n",
      "Epoch 1 Loss: 0.038936447352170944\n",
      "Epoch 1 Loss: 0.16437757015228271\n",
      "Epoch 1 Loss: 0.09489365667104721\n",
      "Epoch 1 Loss: 0.21962665021419525\n",
      "Epoch 1 Loss: 0.17240463197231293\n",
      "Epoch 1 Loss: 0.03873230889439583\n",
      "Epoch 1 Loss: 0.02432831935584545\n",
      "Epoch 1 Loss: 0.0693347305059433\n",
      "Epoch 1 Loss: 0.016665475443005562\n",
      "Epoch 1 Loss: 0.15078845620155334\n",
      "Epoch 1 Loss: 0.03430814668536186\n",
      "Epoch 1 Loss: 0.012701163999736309\n",
      "Epoch 1 Loss: 0.05366303771734238\n",
      "Epoch 1 Loss: 0.07719026505947113\n",
      "Epoch 1 Loss: 0.14806440472602844\n",
      "Epoch 1 Loss: 0.011349462904036045\n",
      "Epoch 1 Loss: 0.11408958584070206\n",
      "Epoch 1 Loss: 0.22740694880485535\n",
      "Epoch 1 Loss: 0.10680632293224335\n",
      "Epoch 1 Loss: 0.1590229868888855\n",
      "Epoch 1 Loss: 0.14095290005207062\n",
      "Epoch 1 Loss: 0.009533396922051907\n",
      "Epoch 1 Loss: 0.017625387758016586\n",
      "Epoch 1 Loss: 0.023857589811086655\n",
      "Epoch 1 Loss: 0.20481814444065094\n",
      "Epoch 1 Loss: 0.33812758326530457\n",
      "Epoch 1 Loss: 0.3409293293952942\n",
      "Epoch 1 Loss: 0.01139720156788826\n",
      "Epoch 1 Loss: 0.2094743549823761\n",
      "Epoch 1 Loss: 0.06795785576105118\n",
      "Epoch 1 Loss: 0.13867591321468353\n",
      "Epoch 1 Loss: 0.2559312582015991\n",
      "Epoch 1 Loss: 0.08956746757030487\n",
      "Epoch 1 Loss: 0.021681616082787514\n",
      "Epoch 1 Loss: 0.17923720180988312\n",
      "Epoch 1 Loss: 0.29814258217811584\n",
      "Epoch 1 Loss: 0.04114045202732086\n",
      "Epoch 1 Loss: 0.13868361711502075\n",
      "Epoch 1 Loss: 0.16154158115386963\n",
      "Epoch 1 Loss: 0.2504771947860718\n",
      "Epoch 1 Loss: 0.06553193926811218\n",
      "Epoch 1 Loss: 0.5141175389289856\n",
      "Epoch 1 Loss: 0.06884145736694336\n",
      "Epoch 1 Loss: 0.016157355159521103\n",
      "Epoch 1 Loss: 0.05772135406732559\n",
      "Epoch 1 Loss: 0.263700932264328\n",
      "Epoch 1 Loss: 0.011096370406448841\n",
      "Epoch 1 Loss: 0.03201233223080635\n",
      "Epoch 1 Loss: 0.11368535459041595\n",
      "Epoch 1 Loss: 0.07857082784175873\n",
      "Epoch 1 Loss: 0.009602474048733711\n",
      "Epoch 1 Loss: 0.21288494765758514\n",
      "Epoch 1 Loss: 0.09072160720825195\n",
      "Epoch 1 Loss: 0.005452003795653582\n",
      "Epoch 1 Loss: 0.12723657488822937\n",
      "Epoch 1 Loss: 0.020042557269334793\n",
      "Epoch 1 Loss: 0.271517276763916\n",
      "Epoch 1 Loss: 0.06674803793430328\n",
      "Epoch 1 Loss: 0.07835610955953598\n",
      "Epoch 1 Loss: 0.31466639041900635\n",
      "Epoch 1 Loss: 0.09434877336025238\n",
      "Epoch 1 Loss: 0.0511704683303833\n",
      "Epoch 1 Loss: 0.26326900720596313\n",
      "Epoch 1 Loss: 0.10702725499868393\n",
      "Epoch 1 Loss: 0.4833124279975891\n",
      "Epoch 1 Loss: 0.20276999473571777\n",
      "Epoch 1 Loss: 0.06889986991882324\n",
      "Epoch 1 Loss: 0.03760642930865288\n",
      "Epoch 1 Loss: 0.1455061435699463\n",
      "Epoch 1 Loss: 0.023480277508497238\n",
      "Epoch 1 Loss: 0.040170975029468536\n",
      "Epoch 1 Loss: 0.021533316001296043\n",
      "Epoch 1 Loss: 0.008020613342523575\n",
      "Epoch 1 Loss: 0.12171465903520584\n",
      "Epoch 1 Loss: 0.31038039922714233\n",
      "Epoch 1 Loss: 0.33526214957237244\n",
      "Epoch 1 Loss: 0.4908878207206726\n",
      "Epoch 1 Loss: 0.28271543979644775\n",
      "Epoch 1 Loss: 0.23780839145183563\n",
      "Epoch 1 Loss: 0.12173666805028915\n",
      "Epoch 1 Loss: 0.09033707529306412\n",
      "Epoch 1 Loss: 0.10670541971921921\n",
      "Epoch 1 Loss: 0.19141118228435516\n",
      "Epoch 1 Loss: 0.19808757305145264\n",
      "Epoch 1 Loss: 0.061134111136198044\n",
      "Epoch 1 Loss: 0.04659835994243622\n",
      "Epoch 1 Loss: 0.14481563866138458\n",
      "Epoch 1 Loss: 0.13760922849178314\n",
      "Epoch 1 Loss: 0.15243622660636902\n",
      "Epoch 1 Loss: 0.172037735581398\n",
      "Epoch 1 Loss: 0.18432508409023285\n",
      "Epoch 1 Loss: 0.09698235243558884\n",
      "Epoch 1 Loss: 0.20101873576641083\n",
      "Epoch 1 Loss: 0.11943426728248596\n",
      "Epoch 1 Loss: 0.20086906850337982\n",
      "Epoch 1 Loss: 0.03856627270579338\n",
      "Epoch 1 Loss: 0.02480165660381317\n",
      "Epoch 1 Loss: 0.28235965967178345\n",
      "Epoch 1 Loss: 0.2613470256328583\n",
      "Epoch 1 Loss: 0.2905839681625366\n",
      "Epoch 1 Loss: 0.15151022374629974\n",
      "Epoch 1 Loss: 0.03083663620054722\n",
      "Epoch 1 Loss: 0.04203081130981445\n",
      "Epoch 1 Loss: 0.02604106068611145\n",
      "Epoch 1 Loss: 0.14139606058597565\n",
      "Epoch 1 Loss: 0.2999366223812103\n",
      "Epoch 1 Loss: 0.12670984864234924\n",
      "Epoch 1 Loss: 0.05858264118432999\n",
      "Epoch 1 Loss: 0.09761659801006317\n",
      "Epoch 1 Loss: 0.11836538463830948\n",
      "Epoch 1 Loss: 0.0927533209323883\n",
      "Epoch 1 Loss: 0.23799178004264832\n",
      "Epoch 1 Loss: 0.02419840171933174\n",
      "Epoch 1 Loss: 0.04513365775346756\n",
      "Epoch 1 Loss: 0.08789441734552383\n",
      "Epoch 1 Loss: 0.10237336158752441\n",
      "Epoch 1 Loss: 0.08233000338077545\n",
      "Epoch 1 Loss: 0.036471229046583176\n",
      "Epoch 1 Loss: 0.09192445129156113\n",
      "Epoch 1 Loss: 0.06376732140779495\n",
      "Epoch 1 Loss: 0.04738780856132507\n",
      "Epoch 1 Loss: 0.07488488405942917\n",
      "Epoch 1 Loss: 0.20428234338760376\n",
      "Epoch 1 Loss: 0.2308644950389862\n",
      "Epoch 1 Loss: 0.0519464872777462\n",
      "Epoch 1 Loss: 0.05992123484611511\n",
      "Epoch 1 Loss: 0.04152704402804375\n",
      "Epoch 1 Loss: 0.02760186418890953\n",
      "Epoch 1 Loss: 0.06261282414197922\n",
      "Epoch 1 Loss: 0.032702766358852386\n",
      "Epoch 1 Loss: 0.05034584924578667\n",
      "Epoch 1 Loss: 0.03112071007490158\n",
      "Epoch 1 Loss: 0.003673393512144685\n",
      "Epoch 1 Loss: 0.5473839640617371\n",
      "Epoch 1 Loss: 0.02422286570072174\n",
      "Epoch 1 Loss: 0.025405870750546455\n",
      "Epoch 1 Loss: 0.38792890310287476\n",
      "Epoch 1 Loss: 0.010300875641405582\n",
      "Epoch 1 Loss: 0.35933393239974976\n",
      "Epoch 1 Loss: 0.03352930769324303\n",
      "Epoch 1 Loss: 0.24247422814369202\n",
      "Epoch 1 Loss: 0.058261625468730927\n",
      "Epoch 1 Loss: 0.20374035835266113\n",
      "Epoch 1 Loss: 0.30219823122024536\n",
      "Epoch 1 Loss: 0.02614183910191059\n",
      "Epoch 1 Loss: 0.10929261147975922\n",
      "Epoch 1 Loss: 0.2099652737379074\n",
      "Epoch 1 Loss: 0.11140582710504532\n",
      "Epoch 1 Loss: 0.04734526947140694\n",
      "Epoch 1 Loss: 0.028443321585655212\n",
      "Epoch 1 Loss: 0.07313612848520279\n",
      "Epoch 1 Loss: 0.21743279695510864\n",
      "Epoch 1 Loss: 0.3222033679485321\n",
      "Epoch 1 Loss: 0.17611433565616608\n",
      "Epoch 1 Loss: 0.1963157206773758\n",
      "Epoch 1 Loss: 0.08832764625549316\n",
      "Epoch 1 Loss: 0.05888057127594948\n",
      "Epoch 1 Loss: 0.26553651690483093\n",
      "Epoch 1 Loss: 0.09410298615694046\n",
      "Epoch 1 Loss: 0.08041223138570786\n",
      "Epoch 1 Loss: 0.04049077257514\n",
      "Epoch 1 Loss: 0.038483064621686935\n",
      "Epoch 1 Loss: 0.16008928418159485\n",
      "Epoch 1 Loss: 0.15972138941287994\n",
      "Epoch 1 Loss: 0.07106177508831024\n",
      "Epoch 1 Loss: 0.1246928945183754\n",
      "Epoch 1 Loss: 0.09493386745452881\n",
      "Epoch 1 Loss: 0.010948493145406246\n",
      "Epoch 1 Loss: 0.009806609712541103\n",
      "Epoch 1 Loss: 0.30187201499938965\n",
      "Epoch 1 Loss: 0.04083947837352753\n",
      "Epoch 1 Loss: 0.01425278838723898\n",
      "Epoch 1 Loss: 0.014835785143077374\n",
      "Epoch 1 Loss: 0.1345032900571823\n",
      "Epoch 1 Loss: 0.010822395794093609\n",
      "Epoch 1 Loss: 0.05389624089002609\n",
      "Epoch 1 Loss: 0.005139543674886227\n",
      "Epoch 1 Loss: 0.19272194802761078\n",
      "Epoch 1 Loss: 0.545106828212738\n",
      "Epoch 1 Loss: 0.05033855885267258\n",
      "Epoch 1 Loss: 0.20756381750106812\n",
      "Epoch 1 Loss: 0.4385256767272949\n",
      "Epoch 1 Loss: 0.1091102808713913\n",
      "Epoch 1 Loss: 0.10536037385463715\n",
      "Epoch 1 Loss: 0.014995289966464043\n",
      "Epoch 1 Loss: 0.1172594279050827\n",
      "Epoch 1 Loss: 0.040846046060323715\n",
      "Epoch 1 Loss: 0.3242737948894501\n",
      "Epoch 1 Loss: 0.05925149843096733\n",
      "Epoch 1 Loss: 0.04406414180994034\n",
      "Epoch 1 Loss: 0.12240790575742722\n",
      "Epoch 1 Loss: 0.08686260879039764\n",
      "Epoch 1 Loss: 0.027234284207224846\n",
      "Epoch 1 Loss: 0.2079332023859024\n",
      "Epoch 1 Loss: 0.01943846046924591\n",
      "Epoch 1 Loss: 0.0195218063890934\n",
      "Epoch 1 Loss: 0.052015118300914764\n",
      "Epoch 1 Loss: 0.3951908349990845\n",
      "Epoch 1 Loss: 0.07612960040569305\n",
      "Epoch 1 Loss: 0.05037684738636017\n",
      "Epoch 1 Loss: 0.12711825966835022\n",
      "Epoch 1 Loss: 0.2632897198200226\n",
      "Epoch 1 Loss: 0.197501078248024\n",
      "Epoch 1 Loss: 0.15642327070236206\n",
      "Epoch 1 Loss: 0.2118290215730667\n",
      "Epoch 1 Loss: 0.2574525773525238\n",
      "Epoch 1 Loss: 0.09423871338367462\n",
      "Epoch 1 Loss: 0.2781972587108612\n",
      "Epoch 1 Loss: 0.1615542620420456\n",
      "Epoch 1 Loss: 0.031078996136784554\n",
      "Epoch 1 Loss: 0.10473580658435822\n",
      "Epoch 1 Loss: 0.13606807589530945\n",
      "Epoch 1 Loss: 0.15982085466384888\n",
      "Epoch 1 Loss: 0.057723160833120346\n",
      "Epoch 1 Loss: 0.15587028861045837\n",
      "Epoch 1 Loss: 0.1363365799188614\n",
      "Epoch 1 Loss: 0.038546424359083176\n",
      "Epoch 1 Loss: 0.01689651980996132\n",
      "Epoch 1 Loss: 0.23584634065628052\n",
      "Epoch 1 Loss: 0.08361458033323288\n",
      "Epoch 1 Loss: 0.06535149365663528\n",
      "Epoch 1 Loss: 0.22002412378787994\n",
      "Epoch 1 Loss: 0.04523080587387085\n",
      "Epoch 1 Loss: 0.0820322260260582\n",
      "Epoch 1 Loss: 0.14490589499473572\n",
      "Epoch 1 Loss: 0.10249939560890198\n",
      "Epoch 1 Loss: 0.21004043519496918\n",
      "Epoch 1 Loss: 0.2641127407550812\n",
      "Epoch 1 Loss: 0.1299387663602829\n",
      "Epoch 1 Loss: 0.13066476583480835\n",
      "Epoch 1 Loss: 0.01406438834965229\n",
      "Epoch 1 Loss: 0.08307859301567078\n",
      "Epoch 1 Loss: 0.03767336159944534\n",
      "Epoch 1 Loss: 0.017316382378339767\n",
      "Epoch 1 Loss: 0.012113727629184723\n",
      "Epoch 1 Loss: 0.09794382750988007\n",
      "Epoch 1 Loss: 0.024635033681988716\n",
      "Epoch 1 Loss: 0.057542115449905396\n",
      "Epoch 1 Loss: 0.2471102476119995\n",
      "Epoch 1 Loss: 0.02294803597033024\n",
      "Epoch 1 Loss: 0.05581766739487648\n",
      "Epoch 1 Loss: 0.15781010687351227\n",
      "Epoch 1 Loss: 0.01987067051231861\n",
      "Epoch 1 Loss: 0.0321560762822628\n",
      "Epoch 1 Loss: 0.3275069296360016\n",
      "Epoch 1 Loss: 0.37539300322532654\n",
      "Epoch 1 Loss: 0.08042878657579422\n",
      "Epoch 1 Loss: 0.020448904484510422\n",
      "Epoch 1 Loss: 0.1924867033958435\n",
      "Epoch 1 Loss: 0.21066665649414062\n",
      "Epoch 1 Loss: 0.16662071645259857\n",
      "Epoch 1 Loss: 0.015017537400126457\n",
      "Epoch 1 Loss: 0.14520114660263062\n",
      "Epoch 1 Loss: 0.024072913452982903\n",
      "Epoch 1 Loss: 0.08543699979782104\n",
      "Epoch 1 Loss: 0.2787507176399231\n",
      "Epoch 1 Loss: 0.09283751249313354\n",
      "Epoch 1 Loss: 0.14476414024829865\n",
      "Epoch 1 Loss: 0.4424990117549896\n",
      "Epoch 1 Loss: 0.17355690896511078\n",
      "Epoch 1 Loss: 0.06057806313037872\n",
      "Epoch 1 Loss: 0.2148400843143463\n",
      "Epoch 1 Loss: 0.024114927276968956\n",
      "Epoch 1 Loss: 0.035199642181396484\n",
      "Epoch 1 Loss: 0.25589704513549805\n",
      "Epoch 1 Loss: 0.023146169260144234\n",
      "Epoch 1 Loss: 0.10269530862569809\n",
      "Epoch 1 Loss: 0.09450968354940414\n",
      "Epoch 1 Loss: 0.0697697177529335\n",
      "Epoch 1 Loss: 0.01914859190583229\n",
      "Epoch 1 Loss: 0.09290099143981934\n",
      "Epoch 1 Loss: 0.1786295622587204\n",
      "Epoch 1 Loss: 0.05210873857140541\n",
      "Epoch 1 Loss: 0.03068443574011326\n",
      "Epoch 1 Loss: 0.10336073487997055\n",
      "Epoch 1 Loss: 0.13496077060699463\n",
      "Epoch 1 Loss: 0.024573728442192078\n",
      "Epoch 1 Loss: 0.05088801681995392\n",
      "Epoch 1 Loss: 0.043973732739686966\n",
      "Epoch 1 Loss: 0.0630607083439827\n",
      "Epoch 1 Loss: 0.08002614229917526\n",
      "Epoch 1 Loss: 0.19380363821983337\n",
      "Epoch 1 Loss: 0.13594214618206024\n",
      "Epoch 1 Loss: 0.1116781234741211\n",
      "Epoch 1 Loss: 0.16700240969657898\n",
      "Epoch 1 Loss: 0.16149620711803436\n",
      "Epoch 1 Loss: 0.21456794440746307\n",
      "Epoch 1 Loss: 0.10585889220237732\n",
      "Epoch 1 Loss: 0.16421544551849365\n",
      "Epoch 1 Loss: 0.1751827746629715\n",
      "Epoch 1 Loss: 0.3437891900539398\n",
      "Epoch 1 Loss: 0.06749168038368225\n",
      "Epoch 1 Loss: 0.04204850271344185\n",
      "Epoch 1 Loss: 0.014980418607592583\n",
      "Epoch 1 Loss: 0.3754632771015167\n",
      "Epoch 1 Loss: 0.2229524850845337\n",
      "Epoch 1 Loss: 0.0686163604259491\n",
      "Epoch 1 Loss: 0.033426739275455475\n",
      "Epoch 1 Loss: 0.040484920144081116\n",
      "Epoch 1 Loss: 0.04974745586514473\n",
      "Epoch 1 Loss: 0.2553490102291107\n",
      "Epoch 1 Loss: 0.14991723001003265\n",
      "Epoch 1 Loss: 0.25791072845458984\n",
      "Epoch 1 Loss: 0.29655298590660095\n",
      "Epoch 1 Loss: 0.04690507799386978\n",
      "Epoch 1 Loss: 0.13519160449504852\n",
      "Epoch 1 Loss: 0.2940596640110016\n",
      "Epoch 1 Loss: 0.04013480991125107\n",
      "Epoch 1 Loss: 0.32747137546539307\n",
      "Epoch 1 Loss: 0.20372870564460754\n",
      "Epoch 1 Loss: 0.16549232602119446\n",
      "Epoch 1 Loss: 0.13816256821155548\n",
      "Epoch 1 Loss: 0.007075292989611626\n",
      "Epoch 1 Loss: 0.0452347993850708\n",
      "Epoch 1 Loss: 0.05006662756204605\n",
      "Epoch 1 Loss: 0.3350909352302551\n",
      "Epoch 1 Loss: 0.09050267189741135\n",
      "Epoch 1 Loss: 0.21641018986701965\n",
      "Epoch 1 Loss: 0.12420755624771118\n",
      "Epoch 1 Loss: 0.08585131913423538\n",
      "Epoch 2 Loss: 0.07121424376964569\n",
      "Epoch 2 Loss: 0.0830942764878273\n",
      "Epoch 2 Loss: 0.18617765605449677\n",
      "Epoch 2 Loss: 0.040850039571523666\n",
      "Epoch 2 Loss: 0.03897926211357117\n",
      "Epoch 2 Loss: 0.04885856434702873\n",
      "Epoch 2 Loss: 0.08972090482711792\n",
      "Epoch 2 Loss: 0.07582221925258636\n",
      "Epoch 2 Loss: 0.01067572832107544\n",
      "Epoch 2 Loss: 0.07640351355075836\n",
      "Epoch 2 Loss: 0.10437490046024323\n",
      "Epoch 2 Loss: 0.004781903699040413\n",
      "Epoch 2 Loss: 0.1470555067062378\n",
      "Epoch 2 Loss: 0.03243178501725197\n",
      "Epoch 2 Loss: 0.029606226831674576\n",
      "Epoch 2 Loss: 0.023995811119675636\n",
      "Epoch 2 Loss: 0.022576944902539253\n",
      "Epoch 2 Loss: 0.009993154555559158\n",
      "Epoch 2 Loss: 0.0061106570065021515\n",
      "Epoch 2 Loss: 0.09701341390609741\n",
      "Epoch 2 Loss: 0.023997066542506218\n",
      "Epoch 2 Loss: 0.027235111221671104\n",
      "Epoch 2 Loss: 0.01046321727335453\n",
      "Epoch 2 Loss: 0.007094544358551502\n",
      "Epoch 2 Loss: 0.004501259420067072\n",
      "Epoch 2 Loss: 0.06317851692438126\n",
      "Epoch 2 Loss: 0.0035609824117273092\n",
      "Epoch 2 Loss: 0.11774145811796188\n",
      "Epoch 2 Loss: 0.12044160068035126\n",
      "Epoch 2 Loss: 0.0012948536314070225\n",
      "Epoch 2 Loss: 0.0022631625179201365\n",
      "Epoch 2 Loss: 0.0018915701657533646\n",
      "Epoch 2 Loss: 0.006580085959285498\n",
      "Epoch 2 Loss: 0.0447801910340786\n",
      "Epoch 2 Loss: 0.006322716828435659\n",
      "Epoch 2 Loss: 0.010960187762975693\n",
      "Epoch 2 Loss: 0.0019468837417662144\n",
      "Epoch 2 Loss: 0.0031637202482670546\n",
      "Epoch 2 Loss: 0.003557425457984209\n",
      "Epoch 2 Loss: 0.051001474261283875\n",
      "Epoch 2 Loss: 0.0017394020687788725\n",
      "Epoch 2 Loss: 0.31427130103111267\n",
      "Epoch 2 Loss: 0.36582282185554504\n",
      "Epoch 2 Loss: 0.1789591759443283\n",
      "Epoch 2 Loss: 0.09296497702598572\n",
      "Epoch 2 Loss: 0.004890442825853825\n",
      "Epoch 2 Loss: 0.005205484572798014\n",
      "Epoch 2 Loss: 0.004885295871645212\n",
      "Epoch 2 Loss: 0.005158087704330683\n",
      "Epoch 2 Loss: 0.17422978579998016\n",
      "Epoch 2 Loss: 0.006736237555742264\n",
      "Epoch 2 Loss: 0.003076641820371151\n",
      "Epoch 2 Loss: 0.01339609082788229\n",
      "Epoch 2 Loss: 0.005167826544493437\n",
      "Epoch 2 Loss: 0.019850991666316986\n",
      "Epoch 2 Loss: 0.008487742394208908\n",
      "Epoch 2 Loss: 0.004993243608623743\n",
      "Epoch 2 Loss: 0.04262078180909157\n",
      "Epoch 2 Loss: 0.049203529953956604\n",
      "Epoch 2 Loss: 0.09213905781507492\n",
      "Epoch 2 Loss: 0.07378217577934265\n",
      "Epoch 2 Loss: 0.08692944049835205\n",
      "Epoch 2 Loss: 0.004271681420505047\n",
      "Epoch 2 Loss: 0.02289411798119545\n",
      "Epoch 2 Loss: 0.003121054731309414\n",
      "Epoch 2 Loss: 0.004739459604024887\n",
      "Epoch 2 Loss: 0.4708211123943329\n",
      "Epoch 2 Loss: 0.393046110868454\n",
      "Epoch 2 Loss: 0.02902170456945896\n",
      "Epoch 2 Loss: 0.19681979715824127\n",
      "Epoch 2 Loss: 0.003542457241564989\n",
      "Epoch 2 Loss: 0.05276431888341904\n",
      "Epoch 2 Loss: 0.21330691874027252\n",
      "Epoch 2 Loss: 0.005046468693763018\n",
      "Epoch 2 Loss: 0.06696628034114838\n",
      "Epoch 2 Loss: 0.1925538182258606\n",
      "Epoch 2 Loss: 0.061516668647527695\n",
      "Epoch 2 Loss: 0.10836770385503769\n",
      "Epoch 2 Loss: 0.022486938163638115\n",
      "Epoch 2 Loss: 0.011633721180260181\n",
      "Epoch 2 Loss: 0.012261828407645226\n",
      "Epoch 2 Loss: 0.005859856493771076\n",
      "Epoch 2 Loss: 0.03214240074157715\n",
      "Epoch 2 Loss: 0.011110988445580006\n",
      "Epoch 2 Loss: 0.04022880643606186\n",
      "Epoch 2 Loss: 0.0036022395361214876\n",
      "Epoch 2 Loss: 0.019563062116503716\n",
      "Epoch 2 Loss: 0.029442228376865387\n",
      "Epoch 2 Loss: 0.06913188099861145\n",
      "Epoch 2 Loss: 0.32404956221580505\n",
      "Epoch 2 Loss: 0.007511842530220747\n",
      "Epoch 2 Loss: 0.08354736864566803\n",
      "Epoch 2 Loss: 0.02953830175101757\n",
      "Epoch 2 Loss: 0.1114896759390831\n",
      "Epoch 2 Loss: 0.04640575125813484\n",
      "Epoch 2 Loss: 0.02178836241364479\n",
      "Epoch 2 Loss: 0.056843411177396774\n",
      "Epoch 2 Loss: 0.030897926539182663\n",
      "Epoch 2 Loss: 0.13550716638565063\n",
      "Epoch 2 Loss: 0.026875421404838562\n",
      "Epoch 2 Loss: 0.029316628351807594\n",
      "Epoch 2 Loss: 0.016645479947328568\n",
      "Epoch 2 Loss: 0.011223727837204933\n",
      "Epoch 2 Loss: 0.03968249261379242\n",
      "Epoch 2 Loss: 0.0624772310256958\n",
      "Epoch 2 Loss: 0.017896641045808792\n",
      "Epoch 2 Loss: 0.041869647800922394\n",
      "Epoch 2 Loss: 0.07348604500293732\n",
      "Epoch 2 Loss: 0.03928127884864807\n",
      "Epoch 2 Loss: 0.26077133417129517\n",
      "Epoch 2 Loss: 0.03421187028288841\n",
      "Epoch 2 Loss: 0.01047253143042326\n",
      "Epoch 2 Loss: 0.008282555267214775\n",
      "Epoch 2 Loss: 0.023262085393071175\n",
      "Epoch 2 Loss: 0.0054560997523367405\n",
      "Epoch 2 Loss: 0.11207043379545212\n",
      "Epoch 2 Loss: 0.12647326290607452\n",
      "Epoch 2 Loss: 0.005481180734932423\n",
      "Epoch 2 Loss: 0.08336679637432098\n",
      "Epoch 2 Loss: 0.057968560606241226\n",
      "Epoch 2 Loss: 0.23022158443927765\n",
      "Epoch 2 Loss: 0.002494845539331436\n",
      "Epoch 2 Loss: 0.012698684819042683\n",
      "Epoch 2 Loss: 0.004828496836125851\n",
      "Epoch 2 Loss: 0.018262453377246857\n",
      "Epoch 2 Loss: 0.002722271950915456\n",
      "Epoch 2 Loss: 0.048207372426986694\n",
      "Epoch 2 Loss: 0.059521716088056564\n",
      "Epoch 2 Loss: 0.060476914048194885\n",
      "Epoch 2 Loss: 0.19119016826152802\n",
      "Epoch 2 Loss: 0.030576009303331375\n",
      "Epoch 2 Loss: 0.015410848893225193\n",
      "Epoch 2 Loss: 0.011657044291496277\n",
      "Epoch 2 Loss: 0.0018039114074781537\n",
      "Epoch 2 Loss: 0.005433692131191492\n",
      "Epoch 2 Loss: 0.29907694458961487\n",
      "Epoch 2 Loss: 0.1065138652920723\n",
      "Epoch 2 Loss: 0.01677769236266613\n",
      "Epoch 2 Loss: 0.044770944863557816\n",
      "Epoch 2 Loss: 0.0020749769173562527\n",
      "Epoch 2 Loss: 0.010852707549929619\n",
      "Epoch 2 Loss: 0.018118349835276604\n",
      "Epoch 2 Loss: 0.0031381966546177864\n",
      "Epoch 2 Loss: 0.13528482615947723\n",
      "Epoch 2 Loss: 0.20524141192436218\n",
      "Epoch 2 Loss: 0.057584524154663086\n",
      "Epoch 2 Loss: 0.3275563716888428\n",
      "Epoch 2 Loss: 0.009258541278541088\n",
      "Epoch 2 Loss: 0.0034295690711587667\n",
      "Epoch 2 Loss: 0.08285737782716751\n",
      "Epoch 2 Loss: 0.004153178073465824\n",
      "Epoch 2 Loss: 0.02039382793009281\n",
      "Epoch 2 Loss: 0.011348191648721695\n",
      "Epoch 2 Loss: 0.017953768372535706\n",
      "Epoch 2 Loss: 0.1436133235692978\n",
      "Epoch 2 Loss: 0.15649938583374023\n",
      "Epoch 2 Loss: 0.06891190260648727\n",
      "Epoch 2 Loss: 0.05637868866324425\n",
      "Epoch 2 Loss: 0.00751114496961236\n",
      "Epoch 2 Loss: 0.026829395443201065\n",
      "Epoch 2 Loss: 0.006207485217601061\n",
      "Epoch 2 Loss: 0.20762796700000763\n",
      "Epoch 2 Loss: 0.01799940876662731\n",
      "Epoch 2 Loss: 0.025214821100234985\n",
      "Epoch 2 Loss: 0.18067562580108643\n",
      "Epoch 2 Loss: 0.04875311255455017\n",
      "Epoch 2 Loss: 0.017326118424534798\n",
      "Epoch 2 Loss: 0.17874549329280853\n",
      "Epoch 2 Loss: 0.003178297309204936\n",
      "Epoch 2 Loss: 0.05248213931918144\n",
      "Epoch 2 Loss: 0.18193861842155457\n",
      "Epoch 2 Loss: 0.05111684650182724\n",
      "Epoch 2 Loss: 0.0036552920937538147\n",
      "Epoch 2 Loss: 0.0028298855759203434\n",
      "Epoch 2 Loss: 0.06246691197156906\n",
      "Epoch 2 Loss: 0.03936748951673508\n",
      "Epoch 2 Loss: 0.04263576492667198\n",
      "Epoch 2 Loss: 0.4162254333496094\n",
      "Epoch 2 Loss: 0.07800708711147308\n",
      "Epoch 2 Loss: 0.0032736577559262514\n",
      "Epoch 2 Loss: 0.006264922674745321\n",
      "Epoch 2 Loss: 0.16797463595867157\n",
      "Epoch 2 Loss: 0.06528060138225555\n",
      "Epoch 2 Loss: 0.019068008288741112\n",
      "Epoch 2 Loss: 0.03914522007107735\n",
      "Epoch 2 Loss: 0.011593902483582497\n",
      "Epoch 2 Loss: 0.0018233199371024966\n",
      "Epoch 2 Loss: 0.006365129724144936\n",
      "Epoch 2 Loss: 0.0216163769364357\n",
      "Epoch 2 Loss: 0.057710085064172745\n",
      "Epoch 2 Loss: 0.1509808450937271\n",
      "Epoch 2 Loss: 0.0058236997574567795\n",
      "Epoch 2 Loss: 0.007848135195672512\n",
      "Epoch 2 Loss: 0.006758044008165598\n",
      "Epoch 2 Loss: 0.01512303575873375\n",
      "Epoch 2 Loss: 0.006082482170313597\n",
      "Epoch 2 Loss: 0.0010476572206243873\n",
      "Epoch 2 Loss: 0.04605623334646225\n",
      "Epoch 2 Loss: 0.003488730639219284\n",
      "Epoch 2 Loss: 0.018345709890127182\n",
      "Epoch 2 Loss: 0.007292005233466625\n",
      "Epoch 2 Loss: 0.23046299815177917\n",
      "Epoch 2 Loss: 0.04036025330424309\n",
      "Epoch 2 Loss: 0.041391532868146896\n",
      "Epoch 2 Loss: 0.07092151045799255\n",
      "Epoch 2 Loss: 0.06580567359924316\n",
      "Epoch 2 Loss: 0.2029576599597931\n",
      "Epoch 2 Loss: 0.04933321103453636\n",
      "Epoch 2 Loss: 0.0042483569122850895\n",
      "Epoch 2 Loss: 0.0029010081198066473\n",
      "Epoch 2 Loss: 0.019292449578642845\n",
      "Epoch 2 Loss: 0.10145127028226852\n",
      "Epoch 2 Loss: 0.03115927428007126\n",
      "Epoch 2 Loss: 0.026066111400723457\n",
      "Epoch 2 Loss: 0.017482681199908257\n",
      "Epoch 2 Loss: 0.01562732830643654\n",
      "Epoch 2 Loss: 0.011107596568763256\n",
      "Epoch 2 Loss: 0.02486465685069561\n",
      "Epoch 2 Loss: 0.00345482281409204\n",
      "Epoch 2 Loss: 0.05047141760587692\n",
      "Epoch 2 Loss: 0.15809111297130585\n",
      "Epoch 2 Loss: 0.030324896797537804\n",
      "Epoch 2 Loss: 0.0035977582447230816\n",
      "Epoch 2 Loss: 0.017463907599449158\n",
      "Epoch 2 Loss: 0.016817333176732063\n",
      "Epoch 2 Loss: 0.009442283771932125\n",
      "Epoch 2 Loss: 0.045345716178417206\n",
      "Epoch 2 Loss: 0.00630785059183836\n",
      "Epoch 2 Loss: 0.0333278588950634\n",
      "Epoch 2 Loss: 0.026210181415081024\n",
      "Epoch 2 Loss: 0.03884411230683327\n",
      "Epoch 2 Loss: 0.04138629883527756\n",
      "Epoch 2 Loss: 0.0034068955574184656\n",
      "Epoch 2 Loss: 0.0015438275877386332\n",
      "Epoch 2 Loss: 0.0043410989455878735\n",
      "Epoch 2 Loss: 0.011583946645259857\n",
      "Epoch 2 Loss: 0.05710193142294884\n",
      "Epoch 2 Loss: 0.37142834067344666\n",
      "Epoch 2 Loss: 0.038322631269693375\n",
      "Epoch 2 Loss: 0.0981488972902298\n",
      "Epoch 2 Loss: 0.016199637204408646\n",
      "Epoch 2 Loss: 0.06792503595352173\n",
      "Epoch 2 Loss: 0.036907169967889786\n",
      "Epoch 2 Loss: 0.37401604652404785\n",
      "Epoch 2 Loss: 0.0011829258874058723\n",
      "Epoch 2 Loss: 0.19128991663455963\n",
      "Epoch 2 Loss: 0.16172657907009125\n",
      "Epoch 2 Loss: 0.09142131358385086\n",
      "Epoch 2 Loss: 0.005940878298133612\n",
      "Epoch 2 Loss: 0.022136682644486427\n",
      "Epoch 2 Loss: 0.006143764592707157\n",
      "Epoch 2 Loss: 0.05100370943546295\n",
      "Epoch 2 Loss: 0.40811094641685486\n",
      "Epoch 2 Loss: 0.05660632252693176\n",
      "Epoch 2 Loss: 0.027175920084118843\n",
      "Epoch 2 Loss: 0.011846115812659264\n",
      "Epoch 2 Loss: 0.3106617331504822\n",
      "Epoch 2 Loss: 0.021112937480211258\n",
      "Epoch 2 Loss: 0.032907482236623764\n",
      "Epoch 2 Loss: 0.06343942135572433\n",
      "Epoch 2 Loss: 0.003475255100056529\n",
      "Epoch 2 Loss: 0.026998410001397133\n",
      "Epoch 2 Loss: 0.03850790858268738\n",
      "Epoch 2 Loss: 0.059189364314079285\n",
      "Epoch 2 Loss: 0.062244970351457596\n",
      "Epoch 2 Loss: 0.03402847424149513\n",
      "Epoch 2 Loss: 0.014503216370940208\n",
      "Epoch 2 Loss: 0.041517119854688644\n",
      "Epoch 2 Loss: 0.04427427798509598\n",
      "Epoch 2 Loss: 0.008703047409653664\n",
      "Epoch 2 Loss: 0.2653302848339081\n",
      "Epoch 2 Loss: 0.011394248344004154\n",
      "Epoch 2 Loss: 0.011115620844066143\n",
      "Epoch 2 Loss: 0.05696891248226166\n",
      "Epoch 2 Loss: 0.0022429912351071835\n",
      "Epoch 2 Loss: 0.008812838234007359\n",
      "Epoch 2 Loss: 0.13752305507659912\n",
      "Epoch 2 Loss: 0.06490737199783325\n",
      "Epoch 2 Loss: 0.04493981972336769\n",
      "Epoch 2 Loss: 0.11623188853263855\n",
      "Epoch 2 Loss: 0.22008328139781952\n",
      "Epoch 2 Loss: 0.01485278457403183\n",
      "Epoch 2 Loss: 0.020451286807656288\n",
      "Epoch 2 Loss: 0.001482375431805849\n",
      "Epoch 2 Loss: 0.19960595667362213\n",
      "Epoch 2 Loss: 0.07148608565330505\n",
      "Epoch 2 Loss: 0.02404487505555153\n",
      "Epoch 2 Loss: 0.20400470495224\n",
      "Epoch 2 Loss: 0.006597083993256092\n",
      "Epoch 2 Loss: 0.20110276341438293\n",
      "Epoch 2 Loss: 0.03597550839185715\n",
      "Epoch 2 Loss: 0.010265805758535862\n",
      "Epoch 2 Loss: 0.1139417439699173\n",
      "Epoch 2 Loss: 0.04786481708288193\n",
      "Epoch 2 Loss: 0.0434623546898365\n",
      "Epoch 2 Loss: 0.012364108115434647\n",
      "Epoch 2 Loss: 0.026868710294365883\n",
      "Epoch 2 Loss: 0.03155392035841942\n",
      "Epoch 2 Loss: 0.1304110735654831\n",
      "Epoch 2 Loss: 0.08512496948242188\n",
      "Epoch 2 Loss: 0.20134864747524261\n",
      "Epoch 2 Loss: 0.026761718094348907\n",
      "Epoch 2 Loss: 0.0229042861610651\n",
      "Epoch 2 Loss: 0.0032432188745588064\n",
      "Epoch 2 Loss: 0.06901369243860245\n",
      "Epoch 2 Loss: 0.0882503092288971\n",
      "Epoch 2 Loss: 0.24367298185825348\n",
      "Epoch 2 Loss: 0.2221519649028778\n",
      "Epoch 2 Loss: 0.1851620078086853\n",
      "Epoch 2 Loss: 0.017316674813628197\n",
      "Epoch 2 Loss: 0.0054685152135789394\n",
      "Epoch 2 Loss: 0.018231505528092384\n",
      "Epoch 2 Loss: 0.04310005530714989\n",
      "Epoch 2 Loss: 0.2494998723268509\n",
      "Epoch 2 Loss: 0.052091967314481735\n",
      "Epoch 2 Loss: 0.031201621517539024\n",
      "Epoch 2 Loss: 0.06656365096569061\n",
      "Epoch 2 Loss: 0.3277592957019806\n",
      "Epoch 2 Loss: 0.04749602824449539\n",
      "Epoch 2 Loss: 0.06729061901569366\n",
      "Epoch 2 Loss: 0.008467297069728374\n",
      "Epoch 2 Loss: 0.07915810495615005\n",
      "Epoch 2 Loss: 0.02438877522945404\n",
      "Epoch 2 Loss: 0.01924879103899002\n",
      "Epoch 2 Loss: 0.22608928382396698\n",
      "Epoch 2 Loss: 0.12910716235637665\n",
      "Epoch 2 Loss: 0.006334154866635799\n",
      "Epoch 2 Loss: 0.061433106660842896\n",
      "Epoch 2 Loss: 0.016871079802513123\n",
      "Epoch 2 Loss: 0.0062610916793346405\n",
      "Epoch 2 Loss: 0.08381117880344391\n",
      "Epoch 2 Loss: 0.14139828085899353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_model.to(MY_DEVICE)\n",
    "distilbert_model.train()\n",
    "\n",
    "train_loader = DataLoader(train_req_dataset, batch_size=16, shuffle=True)\n",
    "optimizer = AdamW(distilbert_model.parameters(), lr=5e-5)\n",
    "\n",
    "# Train for 3 epochs\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        # print(batch)\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(MY_DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(MY_DEVICE)\n",
    "        labels = batch['labels'].to(MY_DEVICE)\n",
    "        outputs = distilbert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch} Loss: {loss.item()}\")\n",
    "\n",
    "distilbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_model.save_pretrained(\"../../../Models/requirement_relevancy_experiment/NLP_models/custom_trained_distilbert_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model (With Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../../../Models/requirement_relevancy_experiment/NLP_models/custom_trained_distilbert_2',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay           # directory for storing logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(distilbert_model.parameters(), lr=5e-5)\n",
    "optimizer_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027ae118e6944b22afb467bff2e3f21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6803, 'learning_rate': 0.0, 'epoch': 1.51}\n",
      "{'train_runtime': 11983.6845, 'train_samples_per_second': 1.328, 'train_steps_per_second': 0.083, 'train_loss': 0.6805132823775571, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=996, training_loss=0.6805132823775571, metrics={'train_runtime': 11983.6845, 'train_samples_per_second': 1.328, 'train_steps_per_second': 0.083, 'train_loss': 0.6805132823775571, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=distilbert_model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_req_dataset,         # training dataset\n",
    "    eval_dataset=valid_req_dataset,             # evaluation dataset\n",
    "    optimizers=(optimizer, optimizer_scheduler)\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_model.save_pretrained(\"../../../Models/requirement_relevancy_experiment/NLP_models/custom_trained_distilbert_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
