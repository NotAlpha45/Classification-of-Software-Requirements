{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(MY_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"caching_allocator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reqs_statement</th>\n",
       "      <th>action_part</th>\n",
       "      <th>actor_part</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user submit job associate cost execution time ...</td>\n",
       "      <td>submit job associate cost execution time deadline</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user establish cost unit time and submit job</td>\n",
       "      <td>establish cost unit time and submit job</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user monitor job submit status</td>\n",
       "      <td>monitor job submit status</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user cancel job submit</td>\n",
       "      <td>cancel job submit</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user check credit balance</td>\n",
       "      <td>check credit balance</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      reqs_statement  \\\n",
       "0  user submit job associate cost execution time ...   \n",
       "1       user establish cost unit time and submit job   \n",
       "2                     user monitor job submit status   \n",
       "3                             user cancel job submit   \n",
       "4                          user check credit balance   \n",
       "\n",
       "                                         action_part actor_part     label  \n",
       "0  submit job associate cost execution time deadline       user  relevant  \n",
       "1            establish cost unit time and submit job       user  relevant  \n",
       "2                          monitor job submit status       user  relevant  \n",
       "3                                  cancel job submit       user  relevant  \n",
       "4                               check credit balance       user  relevant  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requirement_relevancy_dataset = pd.read_csv(\n",
    "    \"../../Datasets/irrelevant_requirements_dataset/irrelevant_requirements_dataset.csv\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "\n",
    "requirement_relevancy_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment With NLP Models\n",
    "\n",
    "In this segment, I will be experimenting with different NLP models to see which one performs the best. I will be using the following models: DistilBERT, ROBERA, DistilBERT, and XLNet. I will be using the HuggingFace library to implement these models. I will be using the same data as the previous notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT Model\n",
    "\n",
    "DistilBERT is a smaller version of BERT. It is trained to be faster and more efficient than BERT. It is also trained to be more memory efficient. It is trained using the same data as BERT. It is trained using a technique called knowledge distillation. This technique is used to compress a large model into a smaller model. The smaller model is trained to mimic the behavior of the larger model. The smaller model is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DistilBertModel,\n",
    "    DistilBertTokenizer,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_X = requirement_relevancy_dataset[\"action_part\"]\n",
    "label_data_y = requirement_relevancy_dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tokenized_text_data_X = bert_tokenizer(\n",
    "        text_data_X.tolist(),\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    tokenized_text_data_y = bert_tokenizer(\n",
    "        label_data_y.tolist(),\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_data_y = {\n",
    "    key: val.to(MY_DEVICE) for key, val in tokenized_text_data_y.items()\n",
    "}\n",
    "\n",
    "tokenized_text_data_X = {\n",
    "    key: val.to(MY_DEVICE) for key, val in tokenized_text_data_X.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([621, 64]), torch.Size([621, 64]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text_data_X[\"input_ids\"].shape, tokenized_text_data_X[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7882,   102,  ...,     0,     0,     0],\n",
       "         [  101,  7882,   102,  ...,     0,     0,     0],\n",
       "         [  101,  7882,   102,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  7882,   102,  ...,     0,     0,     0],\n",
       "         [  101, 22537,   102,  ...,     0,     0,     0],\n",
       "         [  101, 22537,   102,  ...,     0,     0,     0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = DistilBertModel.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    device_map=MY_DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save_pretrained(\n",
    "    \"../../Models/requirement_relevancy_experiment/NLP_models/my_bert_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Gradiant Calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.no_grad() is used to disable gradient calculation because we are not updating the parameters of the model. This will reduce memory consumption for computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    outputs = bert_model(**tokenized_text_data_X)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "# outputs = bert_model(**tokenized_text_data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3534, -0.1981, -0.1656,  ..., -0.3108, -0.0599,  0.4349],\n",
       "         [ 0.1703,  0.1611,  0.2477,  ..., -0.1779,  0.0796,  0.2220],\n",
       "         [-0.0327, -0.2060,  0.1768,  ..., -0.1925, -0.0997,  0.1481],\n",
       "         ...,\n",
       "         [-0.0700,  0.0444,  0.3540,  ..., -0.1894, -0.3079,  0.1799],\n",
       "         [-0.0373,  0.0923,  0.1830,  ..., -0.1250, -0.4743,  0.1674],\n",
       "         [-0.0203,  0.0741,  0.1860,  ..., -0.1342, -0.4503,  0.2178]],\n",
       "\n",
       "        [[-0.2828, -0.0325, -0.4504,  ..., -0.3080,  0.2259,  0.4418],\n",
       "         [-0.0231,  0.4239, -0.1768,  ..., -0.4183,  0.1507,  0.1583],\n",
       "         [ 0.3004, -0.2843,  0.0732,  ..., -0.2676,  0.0906,  0.0983],\n",
       "         ...,\n",
       "         [ 0.1640,  0.0182,  0.3581,  ..., -0.2948, -0.0981,  0.1500],\n",
       "         [-0.0513,  0.0541, -0.0595,  ..., -0.1478, -0.3698,  0.0287],\n",
       "         [-0.0303,  0.0941, -0.0675,  ..., -0.1470, -0.4249,  0.0290]],\n",
       "\n",
       "        [[-0.2093, -0.1249, -0.3169,  ..., -0.2322,  0.0539,  0.3074],\n",
       "         [ 0.4184,  0.3295,  0.0380,  ..., -0.4043,  0.3285, -0.2612],\n",
       "         [ 0.0921, -0.1866,  0.1095,  ..., -0.1873, -0.0347,  0.0763],\n",
       "         ...,\n",
       "         [ 0.0327, -0.0762,  0.0959,  ..., -0.1562, -0.4021,  0.2249],\n",
       "         [ 0.0368, -0.0307,  0.1120,  ..., -0.1959, -0.3422,  0.1510],\n",
       "         [ 0.0643, -0.0090,  0.1581,  ..., -0.1774, -0.3586,  0.0846]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.2975, -0.4075,  0.2748,  ..., -0.3990, -0.0827,  0.4047],\n",
       "         [-0.7869, -0.2370,  0.2266,  ..., -0.1336,  0.3059, -0.1792],\n",
       "         [-0.6847, -0.1531,  0.3300,  ..., -0.5358, -0.3390,  0.1972],\n",
       "         ...,\n",
       "         [-0.1109,  0.2521,  0.7037,  ..., -0.3364, -0.4235,  0.1221],\n",
       "         [ 0.0012, -0.6308,  0.1603,  ..., -0.3522, -0.3822, -0.0900],\n",
       "         [ 0.7799, -0.1031, -0.2255,  ...,  0.0361, -0.6294, -0.2578]],\n",
       "\n",
       "        [[-0.3775, -0.2907,  0.1604,  ..., -0.1742, -0.0041,  0.3944],\n",
       "         [-0.7358, -0.4744, -0.0182,  ...,  0.0270,  0.1651, -0.3535],\n",
       "         [-0.3886, -0.5322, -0.1630,  ..., -0.2945,  0.4871,  0.1024],\n",
       "         ...,\n",
       "         [-0.0773, -0.2172,  0.2278,  ..., -0.1482,  0.0181,  0.2822],\n",
       "         [-0.2944, -0.3239, -0.1919,  ..., -0.0049, -0.0501,  0.1265],\n",
       "         [-0.0635,  0.0103, -0.0184,  ..., -0.2610, -0.0084,  0.3168]],\n",
       "\n",
       "        [[-0.2941, -0.2762,  0.1179,  ..., -0.3106,  0.2294,  0.4937],\n",
       "         [ 0.1255,  0.1660,  0.3551,  ..., -0.0383, -0.0120, -0.1287],\n",
       "         [-0.7412, -0.1886,  0.6405,  ..., -0.3723, -0.0435, -0.0306],\n",
       "         ...,\n",
       "         [-0.0501, -0.0652,  0.3237,  ..., -0.1400, -0.1367,  0.0682],\n",
       "         [-0.0718, -0.1831,  0.3606,  ..., -0.1458, -0.0264,  0.2311],\n",
       "         [-0.2582, -0.2415,  0.2720,  ..., -0.2043,  0.0319,  0.2082]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(621, 49152)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_last_hidden_states_X = (\n",
    "    last_hidden_states.reshape(last_hidden_states.shape[0], -1).detach().cpu().numpy()\n",
    ")\n",
    "reshaped_last_hidden_states_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling Of Data\n",
    "\n",
    "The dataset is pretty imbalanced. So, we will oversample the data to make it balanced. We are currently analyzing various oversampling techniques. We will use the best one for our model. To know more about the various oversampling techniques, please refer to this [link](https://pypi.org/project/smote-variants/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "SMOTE is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It randomly picks a point from the minority class and computes the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smote_variants as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler = sv.MulticlassOversampling(oversampler=\"SMOTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = oversampler.sample(\n",
    "    reshaped_last_hidden_states_X, label_data_y\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
