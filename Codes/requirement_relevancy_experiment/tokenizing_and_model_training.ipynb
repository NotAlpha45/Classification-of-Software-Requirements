{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(MY_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"caching_allocator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reqs_statement</th>\n",
       "      <th>action_part</th>\n",
       "      <th>actor_part</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user submit job associate cost execution time ...</td>\n",
       "      <td>submit job associate cost execution time deadline</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user establish cost unit time and submit job</td>\n",
       "      <td>establish cost unit time and submit job</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user monitor job submit status</td>\n",
       "      <td>monitor job submit status</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user cancel job submit</td>\n",
       "      <td>cancel job submit</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user check credit balance</td>\n",
       "      <td>check credit balance</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      reqs_statement  \\\n",
       "0  user submit job associate cost execution time ...   \n",
       "1       user establish cost unit time and submit job   \n",
       "2                     user monitor job submit status   \n",
       "3                             user cancel job submit   \n",
       "4                          user check credit balance   \n",
       "\n",
       "                                         action_part actor_part     label  \n",
       "0  submit job associate cost execution time deadline       user  relevant  \n",
       "1            establish cost unit time and submit job       user  relevant  \n",
       "2                          monitor job submit status       user  relevant  \n",
       "3                                  cancel job submit       user  relevant  \n",
       "4                               check credit balance       user  relevant  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requirement_relevancy_dataset = pd.read_csv(\n",
    "    \"../../Datasets/irrelevant_requirements_dataset/irrelevant_requirements_dataset.csv\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "\n",
    "requirement_relevancy_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment With NLP Models\n",
    "\n",
    "In this segment, I will be experimenting with different NLP models to see which one performs the best. I will be using the following models: DistilBERT, ROBERA, DistilBERT, and XLNet. I will be using the HuggingFace library to implement these models. I will be using the same data as the previous notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT Model\n",
    "\n",
    "DistilBERT is a smaller version of BERT. It is trained to be faster and more efficient than BERT. It is also trained to be more memory efficient. It is trained using the same data as BERT. It is trained using a technique called knowledge distillation. This technique is used to compress a large model into a smaller model. The smaller model is trained to mimic the behavior of the larger model. The smaller model is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DistilBertModel,\n",
    "    DistilBertTokenizer,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_X = requirement_relevancy_dataset[\"action_part\"]\n",
    "label_data_y = requirement_relevancy_dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tokenized_text_data_X = bert_tokenizer(\n",
    "        text_data_X.tolist(),\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_data_y = np.array(\n",
    "    map(lambda label: 1 if label == \"relevant\" else 0, label_data_y.tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_data_X = {\n",
    "    key: val.to(MY_DEVICE) for key, val in tokenized_text_data_X.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([621, 64]), torch.Size([621, 64]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text_data_X[\"input_ids\"].shape, tokenized_text_data_X[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7882,   102,  ...,     0,     0,     0],\n",
       "         [  101,  7882,   102,  ...,     0,     0,     0],\n",
       "         [  101,  7882,   102,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  7882,   102,  ...,     0,     0,     0],\n",
       "         [  101, 22537,   102,  ...,     0,     0,     0],\n",
       "         [  101, 22537,   102,  ...,     0,     0,     0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = DistilBertModel.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    device_map=MY_DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Precision Calculation\n",
    "\n",
    "Mixed precsion is the use of both 16 and 32 bit float to optimize memory during training to make it run faster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.cuda.amp.autocast() is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    outputs = bert_model(**tokenized_text_data_X)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "# outputs = bert_model(**tokenized_text_data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save_pretrained(\n",
    "    \"../../Models/requirement_relevancy_experiment/NLP_models/my_distilbert_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(621, 49152)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_last_hidden_states_X = (\n",
    "    last_hidden_states.reshape(last_hidden_states.shape[0], -1).detach().cpu().numpy()\n",
    ")\n",
    "reshaped_last_hidden_states_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    \"../../Datasets/irrelevant_requirements_dataset/distilbert_X.csv\",\n",
    "    reshaped_last_hidden_states_X,\n",
    "    delimiter=\",\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the saved DistilBERT model and the reshaped last hidden states\n",
    "\n",
    "# reshaped_last_hidden_states_X = np.loadtxt(\n",
    "#     \"../../Datasets/irrelevant_requirements_dataset/distilbert_X.csv\",\n",
    "#     delimiter=\",\",\n",
    "# )\n",
    "\n",
    "# bert_model = DistilBertModel.from_pretrained(\n",
    "#     \"../../Models/requirement_relevancy_experiment/NLP_models/my_distilbert_model\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling Of Data\n",
    "\n",
    "The dataset is pretty imbalanced. So, we will oversample the data to make it balanced. We are currently analyzing various oversampling techniques. We will use the best one for our model. To know more about the various oversampling techniques, please refer to this [link](https://pypi.org/project/smote-variants/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "SMOTE is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It randomly picks a point from the minority class and computes the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smote_variants as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler = sv.MulticlassOversampling(oversampler=\"SMOTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-13 04:35:22,337:INFO:MulticlassOversampling: Running multiclass oversampling with strategy eq_1_vs_many_successive\n",
      "2024-01-13 04:35:22,389:INFO:MulticlassOversampling: Sampling minority class with label: 0\n",
      "2024-01-13 04:35:22,422:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'nn_params': {}, 'n_jobs': 1, 'ss_params': {'n_dim': 2, 'simplex_sampling': 'random', 'within_simplex_sampling': 'random', 'gaussian_component': {}}, 'random_state': None, 'class_name': 'SMOTE'}\")\n",
      "2024-01-13 04:35:22,428:INFO:NearestNeighborsWithMetricTensor: NN fitting with metric minkowski\n",
      "2024-01-13 04:35:22,432:INFO:NearestNeighborsWithMetricTensor: kneighbors query minkowski\n",
      "2024-01-13 04:35:22,816:INFO:SMOTE: simplex sampling with n_dim 2\n"
     ]
    }
   ],
   "source": [
    "X_resampled, y_resampled = oversampler.sample(\n",
    "    reshaped_last_hidden_states_X, tokenized_text_data_y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Resampling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Irrelevant and Relevant in the total dataset: {'Irrelevant': 64, 'Relevant': 557}\n"
     ]
    }
   ],
   "source": [
    "# count the number of 1 and 0 in the total dataset\n",
    "unique, counts = np.unique(tokenized_text_data_y, return_counts=True)\n",
    "print(\n",
    "    \"Number of Irrelevant and Relevant in the total dataset:\",\n",
    "    dict(zip([\"Irrelevant\", \"Relevant\"], counts)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Resampling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Irrelevant and Relevant in the total dataset: {'Irrelevant': 557, 'Relevant': 557}\n"
     ]
    }
   ],
   "source": [
    "# count the number of 1 and 0 in the total dataset\n",
    "unique, counts = np.unique(y_resampled, return_counts=True)\n",
    "print(\n",
    "    \"Number of Irrelevant and Relevant in the total dataset:\",\n",
    "    dict(zip([\"Irrelevant\", \"Relevant\"], counts)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "In this section we will use various classification models to classify the texts. We will use the output of the hidden layers as the features and the tokenized label of the dataset as training label. We will use ensemble models as they are more robust in classification.\n",
    "\n",
    "Ensemble models are machine learning techniques that combine the predictions of multiple base models to improve overall performance. The key idea is that combining the strengths of different models can lead to a more robust and accurate prediction. Ensemble models are often more accurate than single models because they are less likely to be affected by bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "**_How it works:_** A Random Forest is an ensemble of decision trees trained on random subsets of the features and the training data. Each tree independently makes a prediction, and the final prediction is obtained through voting or averaging.\n",
    "\n",
    "**_Advantages_**: Reduces overfitting, improves stability, and increases accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9910313901345291 \n",
      "Precision score: 0.9917355371900827 \n",
      "Recall score: 0.9917355371900827 \n",
      "f1 score: 0.9917355371900827\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model through various metrics: accuracy, precision, recall, f1-score\n",
    "\n",
    "y_pred = random_forest_classifier.predict(X_test)\n",
    "\n",
    "print(\n",
    "    \"Accuracy score:\",\n",
    "    accuracy_score(y_test, y_pred),\n",
    "    \"\\nPrecision score:\",\n",
    "    precision_score(y_test, y_pred),\n",
    "    \"\\nRecall score:\",\n",
    "    recall_score(y_test, y_pred),\n",
    "    \"\\nf1 score:\",\n",
    "    f1_score(y_test, y_pred),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification for Random Forest\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       102\n",
      "           1       0.99      0.99      0.99       121\n",
      "\n",
      "    accuracy                           0.99       223\n",
      "   macro avg       0.99      0.99      0.99       223\n",
      "weighted avg       0.99      0.99      0.99       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification for Random Forest\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../Models/requirement_relevancy_experiment/classifier_models/distilbert_random_forest_classifier.joblib']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(\n",
    "    random_forest_classifier,\n",
    "    \"../../Models/requirement_relevancy_experiment/classifier_models/distilbert_random_forest_classifier.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Classifier\n",
    "\n",
    "Gradient Boost Classifier is an ensemble model that uses decision trees to classify the data. It uses the boosting technique to create multiple decision trees and then uses the majority vote to classify the data. It is a robust model that is not prone to overfitting. It is also very fast to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_classifier = GradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(random_state=42)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_boosting_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 1.0 \n",
      "Precision score: 1.0 \n",
      "Recall score: 1.0 \n",
      "f1 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model through various metrics: accuracy, precision, recall, f1-score\n",
    "\n",
    "y_pred = gradient_boosting_classifier.predict(X_test)\n",
    "\n",
    "print(\n",
    "    \"Accuracy score:\",\n",
    "    accuracy_score(y_test, y_pred),\n",
    "    \"\\nPrecision score:\",\n",
    "    precision_score(y_test, y_pred),\n",
    "    \"\\nRecall score:\",\n",
    "    recall_score(y_test, y_pred),\n",
    "    \"\\nf1 score:\",\n",
    "    f1_score(y_test, y_pred),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification for Gradient Boosting\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       102\n",
      "           1       1.00      1.00      1.00       121\n",
      "\n",
      "    accuracy                           1.00       223\n",
      "   macro avg       1.00      1.00      1.00       223\n",
      "weighted avg       1.00      1.00      1.00       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification for Gradient Boosting\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../Models/requirement_relevancy_experiment/classifier_models/distilbert_gradient_boost_classifier.joblib']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(\n",
    "    gradient_boosting_classifier,\n",
    "    \"../../Models/requirement_relevancy_experiment/classifier_models/distilbert_gradient_boost_classifier.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost Classifier\n",
    "\n",
    "**_How it works:_** AdaBoost is an ensemble learning method that sequentially trains weak learners on weighted datasets, adjusting weights for misclassified instances in each iteration. The final prediction is made by combining the weak learners' predictions, weighted by their accuracy.\n",
    "\n",
    "**_Advantages:_** AdaBoost is adaptable, emphasizing misclassified instances, has few hyperparameters to tune, is versatile with various base learners, avoids overfitting, is effective for binary classification, handles noisy data, and provides an interpretable final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_classifier = AdaBoostClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(random_state=42)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification result for AdaBoost\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       102\n",
      "           1       0.99      0.91      0.95       121\n",
      "\n",
      "    accuracy                           0.95       223\n",
      "   macro avg       0.95      0.95      0.95       223\n",
      "weighted avg       0.95      0.95      0.95       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "\n",
    "print(\"Classification result for AdaBoost\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../Models/requirement_relevancy_experiment/classifier_models/distilbert_adaboost_classifier.joblib']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(\n",
    "    adaboost_classifier,\n",
    "    \"../../Models/requirement_relevancy_experiment/classifier_models/distilbert_adaboost_classifier.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost Classifier\n",
    "How it works: CatBoost is a gradient boosting algorithm designed for categorical features. It builds an ensemble of decision trees sequentially, minimizing the loss function using gradient descent. CatBoost efficiently handles categorical variables by applying a specialized processing technique.\n",
    "\n",
    "Advantages: CatBoost provides high accuracy, efficient handling of categorical features without extensive preprocessing, and automatic handling of missing data. It is robust against overfitting, requires minimal hyperparameter tuning, and is suitable for a wide range of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_classifier = CatBoostClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.009807\n",
      "0:\tlearn: 0.6867825\ttotal: 1m 6s\tremaining: 18h 32m 8s\n",
      "1:\tlearn: 0.6806839\ttotal: 1m 54s\tremaining: 15h 53m 47s\n",
      "2:\tlearn: 0.6757625\ttotal: 2m 46s\tremaining: 15h 21m 24s\n",
      "3:\tlearn: 0.6701040\ttotal: 3m 54s\tremaining: 16h 12m 50s\n",
      "4:\tlearn: 0.6639940\ttotal: 4m 37s\tremaining: 15h 21m 9s\n",
      "5:\tlearn: 0.6586213\ttotal: 5m 15s\tremaining: 14h 31m 4s\n",
      "6:\tlearn: 0.6533410\ttotal: 5m 56s\tremaining: 14h 2m 5s\n",
      "7:\tlearn: 0.6487984\ttotal: 6m 35s\tremaining: 13h 36m 40s\n",
      "8:\tlearn: 0.6437558\ttotal: 7m 13s\tremaining: 13h 15m 55s\n",
      "9:\tlearn: 0.6377506\ttotal: 7m 56s\tremaining: 13h 6m 4s\n",
      "10:\tlearn: 0.6329636\ttotal: 8m 34s\tremaining: 12h 50m 37s\n",
      "11:\tlearn: 0.6284873\ttotal: 9m 9s\tremaining: 12h 33m 57s\n",
      "12:\tlearn: 0.6225393\ttotal: 10m\tremaining: 12h 40m 1s\n",
      "13:\tlearn: 0.6181432\ttotal: 10m 40s\tremaining: 12h 31m 29s\n",
      "14:\tlearn: 0.6127153\ttotal: 11m 24s\tremaining: 12h 29m 18s\n",
      "15:\tlearn: 0.6078308\ttotal: 12m 3s\tremaining: 12h 21m 59s\n",
      "16:\tlearn: 0.6028685\ttotal: 12m 48s\tremaining: 12h 20m 36s\n",
      "17:\tlearn: 0.5980892\ttotal: 13m 31s\tremaining: 12h 18m 16s\n",
      "18:\tlearn: 0.5923091\ttotal: 14m 15s\tremaining: 12h 16m 7s\n",
      "19:\tlearn: 0.5883841\ttotal: 14m 52s\tremaining: 12h 8m 35s\n",
      "20:\tlearn: 0.5839957\ttotal: 15m 29s\tremaining: 12h 2m 30s\n",
      "21:\tlearn: 0.5795881\ttotal: 16m 7s\tremaining: 11h 56m 34s\n",
      "22:\tlearn: 0.5755586\ttotal: 16m 42s\tremaining: 11h 50m 2s\n",
      "23:\tlearn: 0.5711741\ttotal: 17m 13s\tremaining: 11h 40m 14s\n",
      "24:\tlearn: 0.5675848\ttotal: 17m 45s\tremaining: 11h 32m 52s\n",
      "25:\tlearn: 0.5623696\ttotal: 18m 28s\tremaining: 11h 31m 56s\n",
      "26:\tlearn: 0.5590172\ttotal: 19m 9s\tremaining: 11h 30m 28s\n",
      "27:\tlearn: 0.5530148\ttotal: 21m 29s\tremaining: 12h 25m 59s\n",
      "28:\tlearn: 0.5478310\ttotal: 23m 55s\tremaining: 13h 20m 58s\n",
      "29:\tlearn: 0.5435844\ttotal: 26m 22s\tremaining: 14h 12m 46s\n",
      "30:\tlearn: 0.5397861\ttotal: 28m 31s\tremaining: 14h 51m 36s\n",
      "31:\tlearn: 0.5363735\ttotal: 29m 31s\tremaining: 14h 53m 12s\n",
      "32:\tlearn: 0.5323250\ttotal: 30m 20s\tremaining: 14h 49m 16s\n",
      "33:\tlearn: 0.5279306\ttotal: 32m 51s\tremaining: 15h 33m 45s\n",
      "34:\tlearn: 0.5238742\ttotal: 35m 8s\tremaining: 16h 8m 56s\n",
      "35:\tlearn: 0.5202963\ttotal: 35m 53s\tremaining: 16h 1m 4s\n",
      "36:\tlearn: 0.5164220\ttotal: 36m 39s\tremaining: 15h 54m 5s\n",
      "37:\tlearn: 0.5127987\ttotal: 37m 26s\tremaining: 15h 47m 49s\n",
      "38:\tlearn: 0.5096996\ttotal: 37m 57s\tremaining: 15h 35m 23s\n",
      "39:\tlearn: 0.5060499\ttotal: 38m 32s\tremaining: 15h 24m 54s\n",
      "40:\tlearn: 0.5015651\ttotal: 39m 7s\tremaining: 15h 14m 59s\n",
      "41:\tlearn: 0.4973263\ttotal: 39m 42s\tremaining: 15h 5m 35s\n",
      "42:\tlearn: 0.4931482\ttotal: 40m 18s\tremaining: 14h 56m 55s\n",
      "43:\tlearn: 0.4894764\ttotal: 41m 2s\tremaining: 14h 51m 38s\n",
      "44:\tlearn: 0.4852267\ttotal: 41m 40s\tremaining: 14h 44m 16s\n",
      "45:\tlearn: 0.4824879\ttotal: 42m 15s\tremaining: 14h 36m 31s\n",
      "46:\tlearn: 0.4783797\ttotal: 43m 55s\tremaining: 14h 50m 48s\n",
      "47:\tlearn: 0.4745693\ttotal: 45m 5s\tremaining: 14h 54m 17s\n",
      "48:\tlearn: 0.4700052\ttotal: 46m 19s\tremaining: 14h 59m 11s\n",
      "49:\tlearn: 0.4671645\ttotal: 48m 25s\tremaining: 15h 20m\n",
      "50:\tlearn: 0.4635462\ttotal: 49m 30s\tremaining: 15h 21m 23s\n",
      "51:\tlearn: 0.4598726\ttotal: 50m 22s\tremaining: 15h 18m 23s\n",
      "52:\tlearn: 0.4562399\ttotal: 52m 26s\tremaining: 15h 36m 57s\n",
      "53:\tlearn: 0.4529192\ttotal: 54m 47s\tremaining: 15h 59m 49s\n",
      "54:\tlearn: 0.4493491\ttotal: 55m 29s\tremaining: 15h 53m 29s\n",
      "55:\tlearn: 0.4467158\ttotal: 56m 5s\tremaining: 15h 45m 32s\n",
      "56:\tlearn: 0.4436427\ttotal: 57m 4s\tremaining: 15h 44m 15s\n",
      "57:\tlearn: 0.4404266\ttotal: 57m 46s\tremaining: 15h 38m 28s\n",
      "58:\tlearn: 0.4369672\ttotal: 58m 28s\tremaining: 15h 32m 29s\n",
      "59:\tlearn: 0.4340977\ttotal: 1h 27s\tremaining: 15h 47m 13s\n",
      "60:\tlearn: 0.4307600\ttotal: 1h 2m 29s\tremaining: 16h 2m\n",
      "61:\tlearn: 0.4283303\ttotal: 1h 4m 34s\tremaining: 16h 16m 54s\n",
      "62:\tlearn: 0.4248866\ttotal: 1h 6m 42s\tremaining: 16h 32m 9s\n",
      "63:\tlearn: 0.4222281\ttotal: 1h 8m 40s\tremaining: 16h 44m 16s\n",
      "64:\tlearn: 0.4195154\ttotal: 1h 10m 23s\tremaining: 16h 52m 29s\n",
      "65:\tlearn: 0.4162105\ttotal: 1h 12m 26s\tremaining: 17h 5m 10s\n",
      "66:\tlearn: 0.4129625\ttotal: 1h 14m 11s\tremaining: 17h 13m 12s\n",
      "67:\tlearn: 0.4102334\ttotal: 1h 14m 58s\tremaining: 17h 7m 37s\n",
      "68:\tlearn: 0.4075992\ttotal: 1h 15m 41s\tremaining: 17h 1m 15s\n",
      "69:\tlearn: 0.4050009\ttotal: 1h 16m 19s\tremaining: 16h 54m 5s\n",
      "70:\tlearn: 0.4016523\ttotal: 1h 16m 50s\tremaining: 16h 45m 26s\n",
      "71:\tlearn: 0.3991781\ttotal: 1h 17m 26s\tremaining: 16h 38m 10s\n",
      "72:\tlearn: 0.3960488\ttotal: 1h 18m 3s\tremaining: 16h 31m 17s\n",
      "73:\tlearn: 0.3926696\ttotal: 1h 18m 35s\tremaining: 16h 23m 29s\n",
      "74:\tlearn: 0.3901223\ttotal: 1h 19m 22s\tremaining: 16h 19m 2s\n",
      "75:\tlearn: 0.3879900\ttotal: 1h 19m 56s\tremaining: 16h 12m\n",
      "76:\tlearn: 0.3845697\ttotal: 1h 20m 28s\tremaining: 16h 4m 36s\n",
      "77:\tlearn: 0.3816904\ttotal: 1h 21m 8s\tremaining: 15h 59m 5s\n",
      "78:\tlearn: 0.3792827\ttotal: 1h 21m 44s\tremaining: 15h 53m 2s\n",
      "79:\tlearn: 0.3769304\ttotal: 1h 22m 17s\tremaining: 15h 46m 26s\n",
      "80:\tlearn: 0.3746947\ttotal: 1h 22m 49s\tremaining: 15h 39m 43s\n",
      "81:\tlearn: 0.3723537\ttotal: 1h 23m 18s\tremaining: 15h 32m 35s\n",
      "82:\tlearn: 0.3695577\ttotal: 1h 23m 56s\tremaining: 15h 27m 27s\n",
      "83:\tlearn: 0.3671553\ttotal: 1h 24m 26s\tremaining: 15h 20m 45s\n",
      "84:\tlearn: 0.3648552\ttotal: 1h 25m 1s\tremaining: 15h 15m 12s\n",
      "85:\tlearn: 0.3625125\ttotal: 1h 25m 30s\tremaining: 15h 8m 43s\n",
      "86:\tlearn: 0.3598168\ttotal: 1h 25m 58s\tremaining: 15h 2m 17s\n",
      "87:\tlearn: 0.3577004\ttotal: 1h 26m 31s\tremaining: 14h 56m 45s\n",
      "88:\tlearn: 0.3551622\ttotal: 1h 27m 6s\tremaining: 14h 51m 35s\n",
      "89:\tlearn: 0.3532378\ttotal: 1h 27m 32s\tremaining: 14h 45m 7s\n",
      "90:\tlearn: 0.3509059\ttotal: 1h 28m\tremaining: 14h 39m 11s\n",
      "91:\tlearn: 0.3487037\ttotal: 1h 28m 33s\tremaining: 14h 34m 3s\n",
      "92:\tlearn: 0.3460625\ttotal: 1h 29m 9s\tremaining: 14h 29m 29s\n",
      "93:\tlearn: 0.3438310\ttotal: 1h 29m 41s\tremaining: 14h 24m 28s\n",
      "94:\tlearn: 0.3416370\ttotal: 1h 30m 18s\tremaining: 14h 20m 14s\n",
      "95:\tlearn: 0.3393338\ttotal: 1h 30m 52s\tremaining: 14h 15m 48s\n",
      "96:\tlearn: 0.3373457\ttotal: 1h 31m 26s\tremaining: 14h 11m 11s\n",
      "97:\tlearn: 0.3353086\ttotal: 1h 31m 59s\tremaining: 14h 6m 45s\n",
      "98:\tlearn: 0.3332324\ttotal: 1h 32m 26s\tremaining: 14h 1m 18s\n",
      "99:\tlearn: 0.3314561\ttotal: 1h 33m 20s\tremaining: 14h 8s\n",
      "100:\tlearn: 0.3292224\ttotal: 1h 34m 2s\tremaining: 13h 57m\n",
      "101:\tlearn: 0.3274065\ttotal: 1h 34m 35s\tremaining: 13h 52m 50s\n",
      "102:\tlearn: 0.3250160\ttotal: 1h 35m 26s\tremaining: 13h 51m 10s\n",
      "103:\tlearn: 0.3229602\ttotal: 1h 36m 1s\tremaining: 13h 47m 15s\n",
      "104:\tlearn: 0.3210605\ttotal: 1h 36m 36s\tremaining: 13h 43m 28s\n",
      "105:\tlearn: 0.3192683\ttotal: 1h 37m 11s\tremaining: 13h 39m 45s\n",
      "106:\tlearn: 0.3172995\ttotal: 1h 37m 45s\tremaining: 13h 35m 53s\n",
      "107:\tlearn: 0.3156149\ttotal: 1h 38m 21s\tremaining: 13h 32m 21s\n",
      "108:\tlearn: 0.3136192\ttotal: 1h 38m 58s\tremaining: 13h 29m 1s\n",
      "109:\tlearn: 0.3115029\ttotal: 1h 39m 36s\tremaining: 13h 25m 58s\n",
      "110:\tlearn: 0.3101617\ttotal: 1h 40m 29s\tremaining: 13h 24m 50s\n",
      "111:\tlearn: 0.3082752\ttotal: 1h 41m 2s\tremaining: 13h 21m 3s\n",
      "112:\tlearn: 0.3063499\ttotal: 1h 41m 43s\tremaining: 13h 18m 30s\n",
      "113:\tlearn: 0.3045467\ttotal: 1h 42m 19s\tremaining: 13h 15m 14s\n",
      "114:\tlearn: 0.3029779\ttotal: 1h 42m 53s\tremaining: 13h 11m 45s\n",
      "115:\tlearn: 0.3010056\ttotal: 1h 43m 29s\tremaining: 13h 8m 37s\n",
      "116:\tlearn: 0.2989152\ttotal: 1h 44m 7s\tremaining: 13h 5m 48s\n",
      "117:\tlearn: 0.2975013\ttotal: 1h 44m 46s\tremaining: 13h 3m 9s\n",
      "118:\tlearn: 0.2954345\ttotal: 1h 45m 27s\tremaining: 13h 44s\n",
      "119:\tlearn: 0.2933574\ttotal: 1h 46m 6s\tremaining: 12h 58m 4s\n",
      "120:\tlearn: 0.2913837\ttotal: 1h 46m 43s\tremaining: 12h 55m 20s\n",
      "121:\tlearn: 0.2896803\ttotal: 1h 47m 24s\tremaining: 12h 52m 57s\n",
      "122:\tlearn: 0.2881659\ttotal: 1h 48m 1s\tremaining: 12h 50m 10s\n",
      "123:\tlearn: 0.2865633\ttotal: 1h 48m 44s\tremaining: 12h 48m 13s\n",
      "124:\tlearn: 0.2849450\ttotal: 1h 49m 23s\tremaining: 12h 45m 41s\n",
      "125:\tlearn: 0.2829728\ttotal: 1h 50m 9s\tremaining: 12h 44m 5s\n",
      "126:\tlearn: 0.2812446\ttotal: 1h 50m 52s\tremaining: 12h 42m 8s\n",
      "127:\tlearn: 0.2795891\ttotal: 1h 51m 33s\tremaining: 12h 39m 57s\n",
      "128:\tlearn: 0.2778291\ttotal: 1h 52m 25s\tremaining: 12h 39m 4s\n",
      "129:\tlearn: 0.2763973\ttotal: 1h 53m 1s\tremaining: 12h 36m 21s\n",
      "130:\tlearn: 0.2748486\ttotal: 1h 53m 34s\tremaining: 12h 33m 22s\n",
      "131:\tlearn: 0.2730745\ttotal: 1h 54m 7s\tremaining: 12h 30m 28s\n",
      "132:\tlearn: 0.2714561\ttotal: 1h 54m 38s\tremaining: 12h 27m 16s\n",
      "133:\tlearn: 0.2699877\ttotal: 1h 55m 8s\tremaining: 12h 24m 10s\n",
      "134:\tlearn: 0.2686550\ttotal: 1h 55m 46s\tremaining: 12h 21m 48s\n",
      "135:\tlearn: 0.2671956\ttotal: 1h 56m 17s\tremaining: 12h 18m 48s\n",
      "136:\tlearn: 0.2654600\ttotal: 1h 56m 56s\tremaining: 12h 16m 38s\n",
      "137:\tlearn: 0.2634781\ttotal: 1h 57m 37s\tremaining: 12h 14m 44s\n",
      "138:\tlearn: 0.2619229\ttotal: 1h 58m 9s\tremaining: 12h 11m 56s\n",
      "139:\tlearn: 0.2604200\ttotal: 1h 58m 42s\tremaining: 12h 9m 15s\n",
      "140:\tlearn: 0.2589895\ttotal: 1h 59m 18s\tremaining: 12h 6m 51s\n",
      "141:\tlearn: 0.2574715\ttotal: 1h 59m 55s\tremaining: 12h 4m 38s\n",
      "142:\tlearn: 0.2558582\ttotal: 2h 1m 21s\tremaining: 12h 7m 19s\n",
      "143:\tlearn: 0.2542306\ttotal: 2h 2m\tremaining: 12h 5m 14s\n",
      "144:\tlearn: 0.2527785\ttotal: 2h 2m 39s\tremaining: 12h 3m 16s\n",
      "145:\tlearn: 0.2515701\ttotal: 2h 3m 17s\tremaining: 12h 1m 9s\n",
      "146:\tlearn: 0.2503744\ttotal: 2h 3m 54s\tremaining: 11h 59m\n",
      "147:\tlearn: 0.2488762\ttotal: 2h 4m 29s\tremaining: 11h 56m 42s\n",
      "148:\tlearn: 0.2473298\ttotal: 2h 5m 10s\tremaining: 11h 54m 57s\n",
      "149:\tlearn: 0.2457865\ttotal: 2h 5m 44s\tremaining: 11h 52m 29s\n",
      "150:\tlearn: 0.2440985\ttotal: 2h 6m 21s\tremaining: 11h 50m 26s\n",
      "151:\tlearn: 0.2421086\ttotal: 2h 6m 59s\tremaining: 11h 48m 30s\n",
      "152:\tlearn: 0.2408686\ttotal: 2h 7m 39s\tremaining: 11h 46m 44s\n",
      "153:\tlearn: 0.2395791\ttotal: 2h 8m 19s\tremaining: 11h 44m 54s\n",
      "154:\tlearn: 0.2386336\ttotal: 2h 8m 58s\tremaining: 11h 43m 7s\n",
      "155:\tlearn: 0.2373514\ttotal: 2h 9m 28s\tremaining: 11h 40m 29s\n",
      "156:\tlearn: 0.2356470\ttotal: 2h 10m 5s\tremaining: 11h 38m 28s\n",
      "157:\tlearn: 0.2345405\ttotal: 2h 10m 42s\tremaining: 11h 36m 33s\n",
      "158:\tlearn: 0.2332623\ttotal: 2h 11m 27s\tremaining: 11h 35m 20s\n",
      "159:\tlearn: 0.2322416\ttotal: 2h 12m 3s\tremaining: 11h 33m 20s\n",
      "160:\tlearn: 0.2309768\ttotal: 2h 12m 42s\tremaining: 11h 31m 32s\n",
      "161:\tlearn: 0.2298229\ttotal: 2h 13m 16s\tremaining: 11h 29m 23s\n",
      "162:\tlearn: 0.2285787\ttotal: 2h 13m 45s\tremaining: 11h 26m 49s\n",
      "163:\tlearn: 0.2272974\ttotal: 2h 14m 14s\tremaining: 11h 24m 17s\n",
      "164:\tlearn: 0.2260243\ttotal: 2h 14m 49s\tremaining: 11h 22m 20s\n",
      "165:\tlearn: 0.2246035\ttotal: 2h 15m 20s\tremaining: 11h 20m\n",
      "166:\tlearn: 0.2236408\ttotal: 2h 15m 51s\tremaining: 11h 17m 37s\n",
      "167:\tlearn: 0.2225033\ttotal: 2h 16m 23s\tremaining: 11h 15m 26s\n",
      "168:\tlearn: 0.2211682\ttotal: 2h 17m 1s\tremaining: 11h 13m 44s\n",
      "169:\tlearn: 0.2199345\ttotal: 2h 17m 37s\tremaining: 11h 11m 57s\n",
      "170:\tlearn: 0.2186299\ttotal: 2h 18m 15s\tremaining: 11h 10m 17s\n",
      "171:\tlearn: 0.2175159\ttotal: 2h 18m 46s\tremaining: 11h 8m 2s\n",
      "172:\tlearn: 0.2162162\ttotal: 2h 19m 20s\tremaining: 11h 6m 5s\n",
      "173:\tlearn: 0.2151170\ttotal: 2h 19m 58s\tremaining: 11h 4m 31s\n",
      "174:\tlearn: 0.2139720\ttotal: 2h 20m 36s\tremaining: 11h 2m 51s\n",
      "175:\tlearn: 0.2129263\ttotal: 2h 21m 14s\tremaining: 11h 1m 14s\n",
      "176:\tlearn: 0.2116743\ttotal: 2h 21m 47s\tremaining: 10h 59m 18s\n",
      "177:\tlearn: 0.2106111\ttotal: 2h 22m 22s\tremaining: 10h 57m 28s\n",
      "178:\tlearn: 0.2094380\ttotal: 2h 22m 57s\tremaining: 10h 55m 41s\n",
      "179:\tlearn: 0.2082887\ttotal: 2h 23m 28s\tremaining: 10h 53m 36s\n",
      "180:\tlearn: 0.2072915\ttotal: 2h 24m\tremaining: 10h 51m 39s\n",
      "181:\tlearn: 0.2060878\ttotal: 2h 24m 31s\tremaining: 10h 49m 33s\n",
      "182:\tlearn: 0.2048894\ttotal: 2h 25m 11s\tremaining: 10h 48m 12s\n",
      "183:\tlearn: 0.2039833\ttotal: 2h 25m 41s\tremaining: 10h 46m 8s\n",
      "184:\tlearn: 0.2029298\ttotal: 2h 26m 14s\tremaining: 10h 44m 16s\n",
      "185:\tlearn: 0.2020382\ttotal: 2h 26m 47s\tremaining: 10h 42m 24s\n",
      "186:\tlearn: 0.2009062\ttotal: 2h 27m 21s\tremaining: 10h 40m 40s\n",
      "187:\tlearn: 0.1996388\ttotal: 2h 27m 58s\tremaining: 10h 39m 9s\n",
      "188:\tlearn: 0.1986293\ttotal: 2h 28m 37s\tremaining: 10h 37m 44s\n",
      "189:\tlearn: 0.1974443\ttotal: 2h 29m 13s\tremaining: 10h 36m 10s\n",
      "190:\tlearn: 0.1964303\ttotal: 2h 29m 48s\tremaining: 10h 34m 29s\n",
      "191:\tlearn: 0.1952168\ttotal: 2h 30m 26s\tremaining: 10h 33m 5s\n",
      "192:\tlearn: 0.1939586\ttotal: 2h 31m 6s\tremaining: 10h 31m 49s\n",
      "193:\tlearn: 0.1931286\ttotal: 2h 31m 36s\tremaining: 10h 29m 54s\n",
      "194:\tlearn: 0.1920189\ttotal: 2h 32m 13s\tremaining: 10h 28m 24s\n",
      "195:\tlearn: 0.1910072\ttotal: 2h 32m 50s\tremaining: 10h 26m 57s\n",
      "196:\tlearn: 0.1897982\ttotal: 2h 33m 27s\tremaining: 10h 25m 30s\n",
      "197:\tlearn: 0.1887077\ttotal: 2h 34m 10s\tremaining: 10h 24m 31s\n",
      "198:\tlearn: 0.1873166\ttotal: 2h 34m 50s\tremaining: 10h 23m 16s\n",
      "199:\tlearn: 0.1862659\ttotal: 2h 35m 31s\tremaining: 10h 22m 7s\n",
      "200:\tlearn: 0.1854315\ttotal: 2h 36m 16s\tremaining: 10h 21m 10s\n",
      "201:\tlearn: 0.1843128\ttotal: 2h 36m 55s\tremaining: 10h 19m 55s\n",
      "202:\tlearn: 0.1833198\ttotal: 2h 37m 39s\tremaining: 10h 18m 57s\n",
      "203:\tlearn: 0.1825419\ttotal: 2h 38m 15s\tremaining: 10h 17m 31s\n",
      "204:\tlearn: 0.1814413\ttotal: 2h 38m 49s\tremaining: 10h 15m 54s\n",
      "205:\tlearn: 0.1805909\ttotal: 2h 39m 23s\tremaining: 10h 14m 23s\n",
      "206:\tlearn: 0.1794572\ttotal: 2h 40m\tremaining: 10h 12m 58s\n",
      "207:\tlearn: 0.1786364\ttotal: 2h 40m 33s\tremaining: 10h 11m 22s\n",
      "208:\tlearn: 0.1777943\ttotal: 2h 41m 6s\tremaining: 10h 9m 43s\n",
      "209:\tlearn: 0.1766806\ttotal: 2h 41m 40s\tremaining: 10h 8m 11s\n",
      "210:\tlearn: 0.1757727\ttotal: 2h 42m 26s\tremaining: 10h 7m 25s\n",
      "211:\tlearn: 0.1749623\ttotal: 2h 43m 6s\tremaining: 10h 6m 14s\n",
      "212:\tlearn: 0.1740505\ttotal: 2h 43m 45s\tremaining: 10h 5m 2s\n",
      "213:\tlearn: 0.1733295\ttotal: 2h 44m 19s\tremaining: 10h 3m 31s\n",
      "214:\tlearn: 0.1723996\ttotal: 2h 44m 52s\tremaining: 10h 2m\n",
      "215:\tlearn: 0.1714689\ttotal: 2h 45m 27s\tremaining: 10h 33s\n",
      "216:\tlearn: 0.1704822\ttotal: 2h 46m 2s\tremaining: 9h 59m 8s\n",
      "217:\tlearn: 0.1696681\ttotal: 2h 46m 35s\tremaining: 9h 57m 34s\n",
      "218:\tlearn: 0.1689534\ttotal: 2h 47m 11s\tremaining: 9h 56m 14s\n",
      "219:\tlearn: 0.1681742\ttotal: 2h 47m 41s\tremaining: 9h 54m 32s\n",
      "220:\tlearn: 0.1673597\ttotal: 2h 48m 15s\tremaining: 9h 53m 4s\n",
      "221:\tlearn: 0.1664337\ttotal: 2h 48m 52s\tremaining: 9h 51m 48s\n",
      "222:\tlearn: 0.1654942\ttotal: 2h 49m 29s\tremaining: 9h 50m 35s\n",
      "223:\tlearn: 0.1645407\ttotal: 2h 50m 3s\tremaining: 9h 49m 8s\n",
      "224:\tlearn: 0.1637108\ttotal: 2h 50m 40s\tremaining: 9h 47m 53s\n",
      "225:\tlearn: 0.1628307\ttotal: 2h 51m 26s\tremaining: 9h 47m 9s\n",
      "226:\tlearn: 0.1619160\ttotal: 2h 51m 55s\tremaining: 9h 45m 26s\n",
      "227:\tlearn: 0.1612471\ttotal: 2h 52m 35s\tremaining: 9h 44m 22s\n",
      "228:\tlearn: 0.1605735\ttotal: 2h 53m 10s\tremaining: 9h 43m 4s\n",
      "229:\tlearn: 0.1598744\ttotal: 2h 53m 45s\tremaining: 9h 41m 41s\n",
      "230:\tlearn: 0.1591715\ttotal: 2h 54m 25s\tremaining: 9h 40m 39s\n",
      "231:\tlearn: 0.1583312\ttotal: 2h 55m 5s\tremaining: 9h 39m 37s\n",
      "232:\tlearn: 0.1577321\ttotal: 2h 55m 44s\tremaining: 9h 38m 32s\n",
      "233:\tlearn: 0.1569283\ttotal: 2h 56m 25s\tremaining: 9h 37m 32s\n",
      "234:\tlearn: 0.1560866\ttotal: 2h 57m 3s\tremaining: 9h 36m 21s\n",
      "235:\tlearn: 0.1552029\ttotal: 2h 57m 38s\tremaining: 9h 35m 4s\n",
      "236:\tlearn: 0.1544892\ttotal: 2h 58m 15s\tremaining: 9h 33m 53s\n",
      "237:\tlearn: 0.1538687\ttotal: 2h 58m 50s\tremaining: 9h 32m 35s\n",
      "238:\tlearn: 0.1532872\ttotal: 2h 59m 21s\tremaining: 9h 31m 4s\n",
      "239:\tlearn: 0.1525830\ttotal: 2h 59m 50s\tremaining: 9h 29m 29s\n",
      "240:\tlearn: 0.1515575\ttotal: 3h 28s\tremaining: 9h 28m 22s\n",
      "241:\tlearn: 0.1505805\ttotal: 3h 1m 6s\tremaining: 9h 27m 15s\n",
      "242:\tlearn: 0.1497380\ttotal: 3h 1m 39s\tremaining: 9h 25m 53s\n",
      "243:\tlearn: 0.1490518\ttotal: 3h 2m 17s\tremaining: 9h 24m 46s\n",
      "244:\tlearn: 0.1482393\ttotal: 3h 2m 59s\tremaining: 9h 23m 54s\n",
      "245:\tlearn: 0.1474562\ttotal: 3h 3m 39s\tremaining: 9h 22m 56s\n",
      "246:\tlearn: 0.1468551\ttotal: 3h 4m 12s\tremaining: 9h 21m 33s\n",
      "247:\tlearn: 0.1461851\ttotal: 3h 4m 44s\tremaining: 9h 20m 10s\n",
      "248:\tlearn: 0.1453200\ttotal: 3h 5m 20s\tremaining: 9h 19m 1s\n",
      "249:\tlearn: 0.1445866\ttotal: 3h 5m 57s\tremaining: 9h 17m 51s\n",
      "250:\tlearn: 0.1439783\ttotal: 3h 6m 30s\tremaining: 9h 16m 32s\n",
      "251:\tlearn: 0.1432730\ttotal: 3h 6m 59s\tremaining: 9h 15m 2s\n",
      "252:\tlearn: 0.1426566\ttotal: 3h 7m 31s\tremaining: 9h 13m 42s\n",
      "253:\tlearn: 0.1420320\ttotal: 3h 8m 7s\tremaining: 9h 12m 30s\n",
      "254:\tlearn: 0.1413986\ttotal: 3h 8m 45s\tremaining: 9h 11m 28s\n",
      "255:\tlearn: 0.1407035\ttotal: 3h 9m 21s\tremaining: 9h 10m 19s\n",
      "256:\tlearn: 0.1401257\ttotal: 3h 9m 52s\tremaining: 9h 8m 55s\n",
      "257:\tlearn: 0.1392969\ttotal: 3h 10m 30s\tremaining: 9h 7m 52s\n",
      "258:\tlearn: 0.1386991\ttotal: 3h 11m 11s\tremaining: 9h 6m 58s\n",
      "259:\tlearn: 0.1379679\ttotal: 3h 11m 46s\tremaining: 9h 5m 50s\n",
      "260:\tlearn: 0.1373037\ttotal: 3h 12m 23s\tremaining: 9h 4m 44s\n",
      "261:\tlearn: 0.1367509\ttotal: 3h 12m 53s\tremaining: 9h 3m 19s\n",
      "262:\tlearn: 0.1360499\ttotal: 3h 13m 33s\tremaining: 9h 2m 25s\n",
      "263:\tlearn: 0.1353752\ttotal: 3h 14m 21s\tremaining: 9h 1m 49s\n",
      "264:\tlearn: 0.1348603\ttotal: 3h 15m 1s\tremaining: 9h 54s\n",
      "265:\tlearn: 0.1342704\ttotal: 3h 15m 34s\tremaining: 8h 59m 40s\n",
      "266:\tlearn: 0.1337903\ttotal: 3h 16m 5s\tremaining: 8h 58m 20s\n",
      "267:\tlearn: 0.1331987\ttotal: 3h 16m 35s\tremaining: 8h 56m 58s\n",
      "268:\tlearn: 0.1324470\ttotal: 3h 17m 11s\tremaining: 8h 55m 51s\n",
      "269:\tlearn: 0.1318324\ttotal: 3h 17m 40s\tremaining: 8h 54m 26s\n",
      "270:\tlearn: 0.1310072\ttotal: 3h 18m 16s\tremaining: 8h 53m 22s\n",
      "271:\tlearn: 0.1304203\ttotal: 3h 18m 54s\tremaining: 8h 52m 21s\n",
      "272:\tlearn: 0.1298646\ttotal: 3h 19m 20s\tremaining: 8h 50m 51s\n",
      "273:\tlearn: 0.1291084\ttotal: 3h 19m 55s\tremaining: 8h 49m 43s\n",
      "274:\tlearn: 0.1285777\ttotal: 3h 20m 43s\tremaining: 8h 49m 12s\n",
      "275:\tlearn: 0.1279840\ttotal: 3h 21m 16s\tremaining: 8h 47m 59s\n",
      "276:\tlearn: 0.1273759\ttotal: 3h 21m 53s\tremaining: 8h 46m 58s\n",
      "277:\tlearn: 0.1267604\ttotal: 3h 22m 26s\tremaining: 8h 45m 46s\n",
      "278:\tlearn: 0.1262576\ttotal: 3h 23m\tremaining: 8h 44m 37s\n",
      "279:\tlearn: 0.1256268\ttotal: 3h 23m 29s\tremaining: 8h 43m 14s\n",
      "280:\tlearn: 0.1251645\ttotal: 3h 24m 3s\tremaining: 8h 42m 8s\n",
      "281:\tlearn: 0.1246801\ttotal: 3h 24m 38s\tremaining: 8h 41m 2s\n",
      "282:\tlearn: 0.1241056\ttotal: 3h 25m 17s\tremaining: 8h 40m 8s\n",
      "283:\tlearn: 0.1236065\ttotal: 3h 25m 48s\tremaining: 8h 38m 53s\n",
      "284:\tlearn: 0.1229589\ttotal: 3h 26m 27s\tremaining: 8h 37m 56s\n",
      "285:\tlearn: 0.1224350\ttotal: 3h 27m 8s\tremaining: 8h 37m 6s\n",
      "286:\tlearn: 0.1219508\ttotal: 3h 27m 42s\tremaining: 8h 36m\n",
      "287:\tlearn: 0.1214112\ttotal: 3h 28m 22s\tremaining: 8h 35m 7s\n",
      "288:\tlearn: 0.1209839\ttotal: 3h 28m 58s\tremaining: 8h 34m 6s\n",
      "289:\tlearn: 0.1204022\ttotal: 3h 29m 32s\tremaining: 8h 32m 59s\n",
      "290:\tlearn: 0.1198398\ttotal: 3h 30m 7s\tremaining: 8h 31m 56s\n",
      "291:\tlearn: 0.1193309\ttotal: 3h 30m 43s\tremaining: 8h 30m 57s\n",
      "292:\tlearn: 0.1187803\ttotal: 3h 31m 20s\tremaining: 8h 29m 58s\n",
      "293:\tlearn: 0.1182464\ttotal: 3h 32m 1s\tremaining: 8h 29m 8s\n",
      "294:\tlearn: 0.1177396\ttotal: 3h 32m 35s\tremaining: 8h 28m 3s\n",
      "295:\tlearn: 0.1171786\ttotal: 3h 33m 14s\tremaining: 8h 27m 10s\n",
      "296:\tlearn: 0.1165001\ttotal: 3h 34m 7s\tremaining: 8h 26m 49s\n",
      "297:\tlearn: 0.1159087\ttotal: 3h 34m 49s\tremaining: 8h 26m 4s\n",
      "298:\tlearn: 0.1154552\ttotal: 3h 35m 35s\tremaining: 8h 25m 26s\n",
      "299:\tlearn: 0.1149088\ttotal: 3h 36m 9s\tremaining: 8h 24m 21s\n",
      "300:\tlearn: 0.1143591\ttotal: 3h 36m 45s\tremaining: 8h 23m 21s\n",
      "301:\tlearn: 0.1138856\ttotal: 3h 37m 29s\tremaining: 8h 22m 40s\n",
      "302:\tlearn: 0.1133676\ttotal: 3h 38m 6s\tremaining: 8h 21m 42s\n",
      "303:\tlearn: 0.1130366\ttotal: 3h 38m 42s\tremaining: 8h 20m 42s\n",
      "304:\tlearn: 0.1126076\ttotal: 3h 39m 18s\tremaining: 8h 19m 44s\n",
      "305:\tlearn: 0.1121114\ttotal: 3h 39m 52s\tremaining: 8h 18m 40s\n",
      "306:\tlearn: 0.1114370\ttotal: 3h 40m 25s\tremaining: 8h 17m 34s\n",
      "307:\tlearn: 0.1109174\ttotal: 3h 40m 56s\tremaining: 8h 16m 23s\n",
      "308:\tlearn: 0.1104097\ttotal: 3h 41m 27s\tremaining: 8h 15m 14s\n",
      "309:\tlearn: 0.1099076\ttotal: 3h 42m 5s\tremaining: 8h 14m 19s\n",
      "310:\tlearn: 0.1093711\ttotal: 3h 42m 48s\tremaining: 8h 13m 35s\n",
      "311:\tlearn: 0.1090093\ttotal: 3h 43m 18s\tremaining: 8h 12m 25s\n",
      "312:\tlearn: 0.1084786\ttotal: 3h 43m 57s\tremaining: 8h 11m 34s\n",
      "313:\tlearn: 0.1079007\ttotal: 3h 44m 34s\tremaining: 8h 10m 38s\n",
      "314:\tlearn: 0.1073662\ttotal: 3h 45m 3s\tremaining: 8h 9m 24s\n",
      "315:\tlearn: 0.1068149\ttotal: 3h 45m 34s\tremaining: 8h 8m 16s\n",
      "316:\tlearn: 0.1064531\ttotal: 3h 46m 7s\tremaining: 8h 7m 11s\n",
      "317:\tlearn: 0.1061260\ttotal: 3h 46m 40s\tremaining: 8h 6m 8s\n",
      "318:\tlearn: 0.1056538\ttotal: 3h 47m 12s\tremaining: 8h 5m 3s\n",
      "319:\tlearn: 0.1052096\ttotal: 3h 47m 39s\tremaining: 8h 3m 47s\n",
      "320:\tlearn: 0.1046961\ttotal: 3h 48m 16s\tremaining: 8h 2m 51s\n",
      "321:\tlearn: 0.1042574\ttotal: 3h 48m 50s\tremaining: 8h 1m 50s\n",
      "322:\tlearn: 0.1038643\ttotal: 3h 49m 28s\tremaining: 8h 59s\n",
      "323:\tlearn: 0.1033801\ttotal: 3h 50m 9s\tremaining: 8h 13s\n",
      "324:\tlearn: 0.1029126\ttotal: 3h 50m 48s\tremaining: 7h 59m 22s\n",
      "325:\tlearn: 0.1024904\ttotal: 3h 51m 25s\tremaining: 7h 58m 27s\n",
      "326:\tlearn: 0.1019795\ttotal: 3h 51m 54s\tremaining: 7h 57m 18s\n",
      "327:\tlearn: 0.1014743\ttotal: 3h 52m 39s\tremaining: 7h 56m 39s\n",
      "328:\tlearn: 0.1010015\ttotal: 3h 53m 15s\tremaining: 7h 55m 44s\n",
      "329:\tlearn: 0.1004986\ttotal: 3h 54m 6s\tremaining: 7h 55m 18s\n",
      "330:\tlearn: 0.1000958\ttotal: 3h 54m 45s\tremaining: 7h 54m 28s\n",
      "331:\tlearn: 0.0996563\ttotal: 3h 55m 24s\tremaining: 7h 53m 40s\n",
      "332:\tlearn: 0.0992290\ttotal: 3h 55m 51s\tremaining: 7h 52m 26s\n",
      "333:\tlearn: 0.0987626\ttotal: 3h 56m 20s\tremaining: 7h 51m 16s\n",
      "334:\tlearn: 0.0984227\ttotal: 3h 57m 4s\tremaining: 7h 50m 36s\n",
      "335:\tlearn: 0.0980262\ttotal: 3h 57m 41s\tremaining: 7h 49m 42s\n",
      "336:\tlearn: 0.0975214\ttotal: 3h 58m 21s\tremaining: 7h 48m 55s\n",
      "337:\tlearn: 0.0971396\ttotal: 3h 58m 58s\tremaining: 7h 48m 3s\n",
      "338:\tlearn: 0.0968084\ttotal: 3h 59m 32s\tremaining: 7h 47m 3s\n",
      "339:\tlearn: 0.0962914\ttotal: 4h 9s\tremaining: 7h 46m 11s\n",
      "340:\tlearn: 0.0958747\ttotal: 4h 37s\tremaining: 7h 45m 1s\n",
      "341:\tlearn: 0.0955137\ttotal: 4h 1m 27s\tremaining: 7h 44m 33s\n",
      "342:\tlearn: 0.0950269\ttotal: 4h 2m\tremaining: 7h 43m 33s\n",
      "343:\tlearn: 0.0946186\ttotal: 4h 2m 37s\tremaining: 7h 42m 40s\n",
      "344:\tlearn: 0.0943270\ttotal: 4h 3m 5s\tremaining: 7h 41m 30s\n",
      "345:\tlearn: 0.0939114\ttotal: 4h 3m 32s\tremaining: 7h 40m 20s\n",
      "346:\tlearn: 0.0934995\ttotal: 4h 4m 6s\tremaining: 7h 39m 22s\n",
      "347:\tlearn: 0.0930962\ttotal: 4h 4m 41s\tremaining: 7h 38m 27s\n",
      "348:\tlearn: 0.0927081\ttotal: 4h 5m 15s\tremaining: 7h 37m 29s\n",
      "349:\tlearn: 0.0923814\ttotal: 4h 6m 24s\tremaining: 7h 37m 36s\n",
      "350:\tlearn: 0.0920162\ttotal: 4h 6m 57s\tremaining: 7h 36m 37s\n",
      "351:\tlearn: 0.0916829\ttotal: 4h 7m 32s\tremaining: 7h 35m 42s\n",
      "352:\tlearn: 0.0912816\ttotal: 4h 8m 5s\tremaining: 7h 34m 42s\n",
      "353:\tlearn: 0.0909429\ttotal: 4h 8m 46s\tremaining: 7h 33m 59s\n",
      "354:\tlearn: 0.0906371\ttotal: 4h 9m 38s\tremaining: 7h 33m 34s\n",
      "355:\tlearn: 0.0902534\ttotal: 4h 10m 18s\tremaining: 7h 32m 47s\n",
      "356:\tlearn: 0.0898723\ttotal: 4h 11m\tremaining: 7h 32m 6s\n",
      "357:\tlearn: 0.0893956\ttotal: 4h 11m 43s\tremaining: 7h 31m 25s\n",
      "358:\tlearn: 0.0890621\ttotal: 4h 12m 22s\tremaining: 7h 30m 37s\n",
      "359:\tlearn: 0.0887493\ttotal: 4h 13m 2s\tremaining: 7h 29m 51s\n",
      "360:\tlearn: 0.0884121\ttotal: 4h 13m 39s\tremaining: 7h 28m 59s\n",
      "361:\tlearn: 0.0880455\ttotal: 4h 14m 18s\tremaining: 7h 28m 12s\n",
      "362:\tlearn: 0.0876984\ttotal: 4h 14m 57s\tremaining: 7h 27m 23s\n",
      "363:\tlearn: 0.0872958\ttotal: 4h 15m 39s\tremaining: 7h 26m 42s\n",
      "364:\tlearn: 0.0869783\ttotal: 4h 16m 26s\tremaining: 7h 26m 7s\n",
      "365:\tlearn: 0.0865790\ttotal: 4h 17m 5s\tremaining: 7h 25m 20s\n",
      "366:\tlearn: 0.0862524\ttotal: 4h 18m 7s\tremaining: 7h 25m 13s\n",
      "367:\tlearn: 0.0858744\ttotal: 4h 18m 46s\tremaining: 7h 24m 25s\n",
      "368:\tlearn: 0.0855418\ttotal: 4h 19m 28s\tremaining: 7h 23m 43s\n",
      "369:\tlearn: 0.0851713\ttotal: 4h 20m 7s\tremaining: 7h 22m 55s\n",
      "370:\tlearn: 0.0848343\ttotal: 4h 20m 46s\tremaining: 7h 22m 6s\n",
      "371:\tlearn: 0.0844515\ttotal: 4h 21m 24s\tremaining: 7h 21m 18s\n",
      "372:\tlearn: 0.0841279\ttotal: 4h 22m 4s\tremaining: 7h 20m 31s\n",
      "373:\tlearn: 0.0838389\ttotal: 4h 22m 41s\tremaining: 7h 19m 41s\n",
      "374:\tlearn: 0.0835108\ttotal: 4h 23m 19s\tremaining: 7h 18m 52s\n",
      "375:\tlearn: 0.0832199\ttotal: 4h 24m 18s\tremaining: 7h 18m 38s\n",
      "376:\tlearn: 0.0829618\ttotal: 4h 25m 6s\tremaining: 7h 18m 6s\n",
      "377:\tlearn: 0.0826108\ttotal: 4h 25m 36s\tremaining: 7h 17m 2s\n",
      "378:\tlearn: 0.0822725\ttotal: 4h 26m 11s\tremaining: 7h 16m 9s\n",
      "379:\tlearn: 0.0819655\ttotal: 4h 26m 55s\tremaining: 7h 15m 30s\n",
      "380:\tlearn: 0.0816067\ttotal: 4h 27m 40s\tremaining: 7h 14m 53s\n",
      "381:\tlearn: 0.0812817\ttotal: 4h 28m 28s\tremaining: 7h 14m 20s\n",
      "382:\tlearn: 0.0809858\ttotal: 4h 29m 10s\tremaining: 7h 13m 37s\n",
      "383:\tlearn: 0.0807166\ttotal: 4h 29m 44s\tremaining: 7h 12m 42s\n",
      "384:\tlearn: 0.0804936\ttotal: 4h 30m 19s\tremaining: 7h 11m 48s\n",
      "385:\tlearn: 0.0801869\ttotal: 4h 30m 57s\tremaining: 7h 11m\n",
      "386:\tlearn: 0.0798847\ttotal: 4h 31m 37s\tremaining: 7h 10m 15s\n",
      "387:\tlearn: 0.0795487\ttotal: 4h 32m 11s\tremaining: 7h 9m 20s\n",
      "388:\tlearn: 0.0791961\ttotal: 4h 32m 50s\tremaining: 7h 8m 33s\n",
      "389:\tlearn: 0.0788286\ttotal: 4h 33m 29s\tremaining: 7h 7m 46s\n",
      "390:\tlearn: 0.0785375\ttotal: 4h 34m 10s\tremaining: 7h 7m 1s\n",
      "391:\tlearn: 0.0782081\ttotal: 4h 34m 50s\tremaining: 7h 6m 16s\n",
      "392:\tlearn: 0.0779339\ttotal: 4h 35m 26s\tremaining: 7h 5m 25s\n",
      "393:\tlearn: 0.0775377\ttotal: 4h 36m 22s\tremaining: 7h 5m 4s\n",
      "394:\tlearn: 0.0772043\ttotal: 4h 36m 56s\tremaining: 7h 4m 10s\n",
      "395:\tlearn: 0.0768844\ttotal: 4h 37m 31s\tremaining: 7h 3m 17s\n",
      "396:\tlearn: 0.0765808\ttotal: 4h 38m 9s\tremaining: 7h 2m 29s\n",
      "397:\tlearn: 0.0763204\ttotal: 4h 38m 46s\tremaining: 7h 1m 39s\n",
      "398:\tlearn: 0.0760471\ttotal: 4h 39m 20s\tremaining: 7h 46s\n",
      "399:\tlearn: 0.0757776\ttotal: 4h 40m\tremaining: 7h\n",
      "400:\tlearn: 0.0755191\ttotal: 4h 40m 40s\tremaining: 6h 59m 15s\n",
      "401:\tlearn: 0.0752181\ttotal: 4h 41m 18s\tremaining: 6h 58m 28s\n",
      "402:\tlearn: 0.0748952\ttotal: 4h 41m 54s\tremaining: 6h 57m 37s\n",
      "403:\tlearn: 0.0745911\ttotal: 4h 42m 28s\tremaining: 6h 56m 43s\n",
      "404:\tlearn: 0.0744014\ttotal: 4h 43m 1s\tremaining: 6h 55m 47s\n",
      "405:\tlearn: 0.0740582\ttotal: 4h 43m 29s\tremaining: 6h 54m 45s\n",
      "406:\tlearn: 0.0737069\ttotal: 4h 44m 6s\tremaining: 6h 53m 57s\n",
      "407:\tlearn: 0.0734605\ttotal: 4h 44m 50s\tremaining: 6h 53m 17s\n",
      "408:\tlearn: 0.0732204\ttotal: 4h 45m 21s\tremaining: 6h 52m 20s\n"
     ]
    }
   ],
   "source": [
    "catboost_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier\n",
    "\n",
    "**_How it works:_** XGBoost is a gradient boosting algorithm that combines the strengths of boosting and regularization techniques. It minimizes a loss function by adding weak learners sequentially and uses gradient descent for optimization.\n",
    "\n",
    "**_Advantages:_** High accuracy, handles missing data, and provides feature importance.\n",
    "\n",
    "More about XGBoost [here](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_classifier = XGBClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification for XG Boosting\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       102\n",
      "           1       0.98      0.98      0.98       121\n",
      "\n",
      "    accuracy                           0.98       223\n",
      "   macro avg       0.98      0.98      0.98       223\n",
      "weighted avg       0.98      0.98      0.98       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model through various metrics: accuracy, precision, recall, f1-score by printing the classification report\n",
    "\n",
    "y_pred = xgboost_classifier.predict(X_test)\n",
    "print(\"Classification for XG Boosting\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../Models/requirement_relevancy_experiment/classifier_models/distilbert_xgboost_classifier.joblib']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(\n",
    "    xgboost_classifier,\n",
    "    \"../../Models/requirement_relevancy_experiment/classifier_models/distilbert_xgboost_classifier.joblib\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
