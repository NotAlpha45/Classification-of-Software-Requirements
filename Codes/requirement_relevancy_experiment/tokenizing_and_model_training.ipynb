{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(MY_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reqs_statement</th>\n",
       "      <th>action_part</th>\n",
       "      <th>actor_part</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user submit job associate cost execution time ...</td>\n",
       "      <td>submit job associate cost execution time deadline</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user establish cost unit time and submit job</td>\n",
       "      <td>establish cost unit time and submit job</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user monitor job submit status</td>\n",
       "      <td>monitor job submit status</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user cancel job submit</td>\n",
       "      <td>cancel job submit</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user check credit balance</td>\n",
       "      <td>check credit balance</td>\n",
       "      <td>user</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      reqs_statement  \\\n",
       "0  user submit job associate cost execution time ...   \n",
       "1       user establish cost unit time and submit job   \n",
       "2                     user monitor job submit status   \n",
       "3                             user cancel job submit   \n",
       "4                          user check credit balance   \n",
       "\n",
       "                                         action_part actor_part     label  \n",
       "0  submit job associate cost execution time deadline       user  relevant  \n",
       "1            establish cost unit time and submit job       user  relevant  \n",
       "2                          monitor job submit status       user  relevant  \n",
       "3                                  cancel job submit       user  relevant  \n",
       "4                               check credit balance       user  relevant  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requirement_relevancy_dataset = pd.read_csv(\n",
    "    \"../../Datasets/irrelevant_requirements_dataset/irrelevant_requirements_dataset.csv\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "\n",
    "requirement_relevancy_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment With NLP Models\n",
    "\n",
    "In this segment, I will be experimenting with different NLP models to see which one performs the best. I will be using the following models: BERT, ROBERA, DistilBERT, and XLNet. I will be using the HuggingFace library to implement these models. I will be using the same data as the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model\n",
    "\n",
    "BERT is a transformer-based model that was proposed in the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805). BERT stands for Bidirectional Encoder Representations from Transformers. BERT is a pre-trained model that can be fine-tuned for a variety of NLP tasks. BERT is a bidirectional model that uses the Transformer encoder architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_X = requirement_relevancy_dataset[\"action_part\"]\n",
    "label_data_y = requirement_relevancy_dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "robert_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_data_X = robert_tokenizer(\n",
    "    text_data_X.tolist(), padding=\"max_length\", return_tensors=\"pt\", max_length=128\n",
    ")\n",
    "tokenized_text_data_X = {\n",
    "    key: val.to(MY_DEVICE) for key, val in tokenized_text_data_X.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_data_y = robert_tokenizer(\n",
    "    label_data_y.tolist(), padding=\"max_length\", return_tensors=\"pt\", max_length=16\n",
    ")\n",
    "\n",
    "tokenized_text_data_y = {\n",
    "    key: val.to(MY_DEVICE) for key, val in tokenized_text_data_y.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bert_model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m      2\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mbert-base-uncased\u001b[39;49m\u001b[39m\"\u001b[39;49m, num_labels\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, device_map\u001b[39m=\u001b[39;49mMY_DEVICE\n\u001b[0;32m      3\u001b[0m )\n",
      "File \u001b[1;32mc:\\SWE Class\\Github Desktop\\Classification of Software Requirements\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:2863\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2859\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2860\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2861\u001b[0m         )\n\u001b[0;32m   2862\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2863\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   2864\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2865\u001b[0m         )\n\u001b[0;32m   2867\u001b[0m quantization_method_from_args \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2869\u001b[0m \u001b[39mif\u001b[39;00m quantization_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2, device_map=MY_DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling Of Data\n",
    "\n",
    "The dataset is pretty imbalanced. So, we will oversample the data to make it balanced. We are currently analyzing various oversampling techniques. We will use the best one for our model. To know more about the various oversampling techniques, please refer to this [link](https://pypi.org/project/smote-variants/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "SMOTE is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It randomly picks a point from the minority class and computes the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smote_variants as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_oversampler = sv.MulticlassOversampling(\n",
    "    oversampler=\"distance_SMOTE\", oversampler_params={\"random_state\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-13 00:37:55,104:INFO:MulticlassOversampling: Running multiclass oversampling with strategy eq_1_vs_many_successive\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m oversamplex_X, oversamplex_y \u001b[39m=\u001b[39m smote_oversampler\u001b[39m.\u001b[39;49msample(\n\u001b[0;32m      2\u001b[0m     tokenized_text_data_X, tokenized_text_data_y\n\u001b[0;32m      3\u001b[0m )\n",
      "File \u001b[1;32mc:\\SWE Class\\Github Desktop\\Classification of Software Requirements\\venv\\Lib\\site-packages\\smote_variants\\multiclassoversampling\\_multiclassoversampling.py:278\u001b[0m, in \u001b[0;36mMulticlassOversampling.sample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[39mDoes the sample generation according to the oversampling strategy.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39m    (np.ndarray, np.array): the extended training set and target labels\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39meq_1_vs_many_successive\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_equalize_1_vs_many_successive(X, y)\n\u001b[0;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_equalize_1_vs_many(X, y)\n",
      "File \u001b[1;32mc:\\SWE Class\\Github Desktop\\Classification of Software Requirements\\venv\\Lib\\site-packages\\smote_variants\\multiclassoversampling\\_multiclassoversampling.py:211\u001b[0m, in \u001b[0;36mMulticlassOversampling.sample_equalize_1_vs_many_successive\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    207\u001b[0m _logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[0;32m    208\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning multiclass oversampling with strategy \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[39m# extract class label statistics\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_label_statistics(y)\n\u001b[0;32m    213\u001b[0m \u001b[39m# sort labels by number of samples\u001b[39;00m\n\u001b[0;32m    214\u001b[0m labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_stats\u001b[39m.\u001b[39mkeys()\n",
      "File \u001b[1;32mc:\\SWE Class\\Github Desktop\\Classification of Software Requirements\\venv\\Lib\\site-packages\\smote_variants\\base\\_base.py:295\u001b[0m, in \u001b[0;36mStatisticsMixin.class_label_statistics\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mdetermines class sizes and minority and majority labels\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[39m    X (np.array): features\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m    y (np.array): target labels\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m unique, counts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y, return_counts\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 295\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_stats \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(unique, counts))\n\u001b[0;32m    296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_label \u001b[39m=\u001b[39m unique[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m counts[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m counts[\u001b[39m1\u001b[39m] \u001b[39melse\u001b[39;00m unique[\u001b[39m1\u001b[39m]\n\u001b[0;32m    297\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaj_label \u001b[39m=\u001b[39m unique[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m counts[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m counts[\u001b[39m1\u001b[39m] \u001b[39melse\u001b[39;00m unique[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "oversamplex_X, oversamplex_y = smote_oversampler.sample(\n",
    "    tokenized_text_data_X, tokenized_text_data_y\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
